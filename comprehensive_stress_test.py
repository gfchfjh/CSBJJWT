"""
KOOKæ¶ˆæ¯è½¬å‘ç³»ç»Ÿ - å…¨é¢ç»¼åˆå‹åŠ›æµ‹è¯•
è¦†ç›–æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½å’Œè¾¹ç•Œæ¡ä»¶
"""
import asyncio
import aiohttp
import time
import random
import json
import sys
import os
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional
import sqlite3
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import multiprocessing

# æ·»åŠ åç«¯è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "backend"))

# æµ‹è¯•é…ç½®
COMPREHENSIVE_TEST_CONFIG = {
    "api_base_url": "http://127.0.0.1:9527",
    "redis_host": "127.0.0.1",
    "redis_port": 6379,
    
    # å¹¶å‘æµ‹è¯•é…ç½®
    "concurrent_users": [1, 5, 10, 25, 50, 100, 200],
    "concurrent_websockets": [1, 5, 10, 20, 50],
    
    # è´Ÿè½½æµ‹è¯•é…ç½®
    "message_batch_sizes": [10, 50, 100, 500, 1000, 5000],
    "sustained_load_duration": 300,  # 5åˆ†é’ŸæŒç»­è´Ÿè½½
    
    # ååé‡æµ‹è¯•é…ç½®
    "throughput_test_duration": 60,  # 1åˆ†é’Ÿååé‡æµ‹è¯•
    "target_qps": [10, 50, 100, 200, 500],
    
    # ç¨³å®šæ€§æµ‹è¯•é…ç½®
    "stability_test_duration": 600,  # 10åˆ†é’Ÿç¨³å®šæ€§æµ‹è¯•
    "stability_request_interval": 0.1,  # æ¯0.1ç§’ä¸€ä¸ªè¯·æ±‚
}


class ComprehensiveStressTest:
    """å…¨é¢ç»¼åˆå‹åŠ›æµ‹è¯•"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.api_base = config["api_base_url"]
        self.session = None
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.total_requests = 0
        self.successful_requests = 0
        self.failed_requests = 0
        self.response_times = []
        self.errors = []
        
        # æ€§èƒ½æŒ‡æ ‡
        self.peak_qps = 0
        self.avg_qps = 0
        self.p50_latency = 0
        self.p90_latency = 0
        self.p99_latency = 0
        
    async def setup(self):
        """åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒ"""
        print("=" * 100)
        print(" " * 30 + "KOOKæ¶ˆæ¯è½¬å‘ç³»ç»Ÿ - å…¨é¢å‹åŠ›æµ‹è¯•")
        print("=" * 100)
        print(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"APIåœ°å€: {self.api_base}")
        print(f"æµ‹è¯•é…ç½®: {len(COMPREHENSIVE_TEST_CONFIG)}ä¸ªæµ‹è¯•ç»´åº¦")
        print("=" * 100)
        print()
        
        timeout = aiohttp.ClientTimeout(total=30)
        self.session = aiohttp.ClientSession(timeout=timeout)
        
        # æ£€æŸ¥æœåŠ¡çŠ¶æ€
        try:
            async with self.session.get(f"{self.api_base}/health") as resp:
                if resp.status == 200:
                    print("âœ… åç«¯æœåŠ¡è¿è¡Œæ­£å¸¸")
                    return True
                else:
                    print("âŒ åç«¯æœåŠ¡å“åº”å¼‚å¸¸")
                    return False
        except Exception as e:
            print(f"âŒ æ— æ³•è¿æ¥åˆ°åç«¯æœåŠ¡: {e}")
            print("è¯·å…ˆå¯åŠ¨åç«¯æœåŠ¡: cd backend && python -m app.main")
            return False
    
    async def teardown(self):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        if self.session:
            await self.session.close()
    
    async def make_request(self, method: str, endpoint: str, **kwargs) -> Dict[str, Any]:
        """å‘é€HTTPè¯·æ±‚"""
        self.total_requests += 1
        start_time = time.time()
        
        try:
            url = f"{self.api_base}{endpoint}"
            async with self.session.request(method, url, **kwargs) as resp:
                response_time = time.time() - start_time
                self.response_times.append(response_time)
                
                if resp.status < 400:
                    self.successful_requests += 1
                else:
                    self.failed_requests += 1
                
                try:
                    data = await resp.json() if resp.content_type == 'application/json' else None
                except:
                    data = None
                
                return {
                    "status": resp.status,
                    "response_time": response_time,
                    "data": data
                }
        except Exception as e:
            response_time = time.time() - start_time
            self.failed_requests += 1
            self.errors.append(str(e))
            return {
                "status": 0,
                "response_time": response_time,
                "error": str(e)
            }
    
    async def test_throughput_under_load(self, target_qps: int) -> Dict[str, Any]:
        """æµ‹è¯•1: ååé‡æµ‹è¯• - ç›®æ ‡QPSä¸‹çš„ç³»ç»Ÿæ€§èƒ½"""
        print(f"\n{'='*100}")
        print(f"æµ‹è¯•1: ååé‡æµ‹è¯• - ç›®æ ‡QPS: {target_qps}")
        print(f"{'='*100}")
        
        duration = self.config["throughput_test_duration"]
        interval = 1.0 / target_qps
        
        print(f"ç›®æ ‡: {target_qps} QPS, æŒç»­æ—¶é—´: {duration}ç§’, è¯·æ±‚é—´éš”: {interval*1000:.2f}ms")
        
        start_time = time.time()
        requests_sent = 0
        request_times = []
        
        endpoints = [
            ("GET", "/api/system/status"),
            ("GET", "/api/accounts"),
            ("GET", "/api/bots"),
            ("GET", "/api/logs?limit=10"),
        ]
        
        while time.time() - start_time < duration:
            iter_start = time.time()
            
            # å‘é€è¯·æ±‚
            method, endpoint = random.choice(endpoints)
            result = await self.make_request(method, endpoint)
            request_times.append(result["response_time"])
            requests_sent += 1
            
            # æ§åˆ¶è¯·æ±‚é€Ÿç‡
            elapsed = time.time() - iter_start
            sleep_time = max(0, interval - elapsed)
            if sleep_time > 0:
                await asyncio.sleep(sleep_time)
        
        total_time = time.time() - start_time
        actual_qps = requests_sent / total_time
        success_rate = sum(1 for t in request_times if t > 0) / len(request_times) * 100
        
        # è®¡ç®—å»¶è¿Ÿç™¾åˆ†ä½
        sorted_times = sorted(request_times)
        p50 = sorted_times[len(sorted_times)//2] * 1000
        p90 = sorted_times[int(len(sorted_times)*0.9)] * 1000
        p99 = sorted_times[int(len(sorted_times)*0.99)] * 1000
        avg = sum(request_times) / len(request_times) * 1000
        
        print(f"\nç»“æœ:")
        print(f"  è¯·æ±‚æ€»æ•°: {requests_sent}")
        print(f"  å®é™…QPS: {actual_qps:.2f}")
        print(f"  æˆåŠŸç‡: {success_rate:.2f}%")
        print(f"  å¹³å‡å»¶è¿Ÿ: {avg:.2f}ms")
        print(f"  P50å»¶è¿Ÿ: {p50:.2f}ms")
        print(f"  P90å»¶è¿Ÿ: {p90:.2f}ms")
        print(f"  P99å»¶è¿Ÿ: {p99:.2f}ms")
        
        return {
            "test_name": f"ååé‡æµ‹è¯• ({target_qps} QPS)",
            "target_qps": target_qps,
            "actual_qps": actual_qps,
            "total_requests": requests_sent,
            "success_rate": success_rate,
            "latency_avg_ms": avg,
            "latency_p50_ms": p50,
            "latency_p90_ms": p90,
            "latency_p99_ms": p99
        }
    
    async def test_sustained_load(self) -> Dict[str, Any]:
        """æµ‹è¯•2: æŒç»­è´Ÿè½½æµ‹è¯• - é•¿æ—¶é—´ç¨³å®šæ€§"""
        print(f"\n{'='*100}")
        print("æµ‹è¯•2: æŒç»­è´Ÿè½½æµ‹è¯• (5åˆ†é’Ÿ)")
        print(f"{'='*100}")
        
        duration = self.config["sustained_load_duration"]
        qps = 50  # ä¸­ç­‰è´Ÿè½½
        
        print(f"è´Ÿè½½: {qps} QPS, æŒç»­: {duration}ç§’ ({duration/60:.1f}åˆ†é’Ÿ)")
        
        start_time = time.time()
        requests_by_minute = []
        current_minute_requests = []
        
        endpoints = [
            ("GET", "/api/system/status"),
            ("GET", "/api/system/stats"),
            ("GET", "/api/logs?limit=20"),
            ("GET", "/api/accounts"),
        ]
        
        while time.time() - start_time < duration:
            # æ¯ç§’å‘é€qpsä¸ªè¯·æ±‚
            tasks = []
            for _ in range(qps):
                method, endpoint = random.choice(endpoints)
                tasks.append(self.make_request(method, endpoint))
            
            results = await asyncio.gather(*tasks)
            current_minute_requests.extend(results)
            
            # æ¯60ç§’ç»Ÿè®¡ä¸€æ¬¡
            if len(current_minute_requests) >= qps * 60:
                success = sum(1 for r in current_minute_requests if r["status"] < 400)
                requests_by_minute.append({
                    "minute": len(requests_by_minute) + 1,
                    "requests": len(current_minute_requests),
                    "success": success,
                    "success_rate": success / len(current_minute_requests) * 100
                })
                current_minute_requests = []
                print(f"  åˆ†é’Ÿ {len(requests_by_minute)}: {success}/{qps*60} æˆåŠŸ")
            
            await asyncio.sleep(1)
        
        # æœ€åä¸è¶³ä¸€åˆ†é’Ÿçš„ç»Ÿè®¡
        if current_minute_requests:
            success = sum(1 for r in current_minute_requests if r["status"] < 400)
            requests_by_minute.append({
                "minute": len(requests_by_minute) + 1,
                "requests": len(current_minute_requests),
                "success": success,
                "success_rate": success / len(current_minute_requests) * 100
            })
        
        total_requests = sum(m["requests"] for m in requests_by_minute)
        total_success = sum(m["success"] for m in requests_by_minute)
        overall_success_rate = total_success / total_requests * 100
        
        print(f"\næŒç»­è´Ÿè½½æµ‹è¯•å®Œæˆ:")
        print(f"  æ€»è¯·æ±‚æ•°: {total_requests}")
        print(f"  æˆåŠŸè¯·æ±‚: {total_success}")
        print(f"  æ•´ä½“æˆåŠŸç‡: {overall_success_rate:.2f}%")
        
        return {
            "test_name": "æŒç»­è´Ÿè½½æµ‹è¯•",
            "duration_seconds": duration,
            "target_qps": qps,
            "total_requests": total_requests,
            "total_success": total_success,
            "overall_success_rate": overall_success_rate,
            "by_minute": requests_by_minute
        }
    
    async def test_spike_load(self) -> Dict[str, Any]:
        """æµ‹è¯•3: å³°å€¼è´Ÿè½½æµ‹è¯• - çªå‘æµé‡å¤„ç†"""
        print(f"\n{'='*100}")
        print("æµ‹è¯•3: å³°å€¼è´Ÿè½½æµ‹è¯• (çªå‘æµé‡)")
        print(f"{'='*100}")
        
        results = []
        
        # æ¨¡æ‹Ÿä¸åŒå¼ºåº¦çš„çªå‘æµé‡
        spike_configs = [
            {"name": "å°çªå‘", "burst_size": 50, "burst_duration": 5},
            {"name": "ä¸­çªå‘", "burst_size": 100, "burst_duration": 10},
            {"name": "å¤§çªå‘", "burst_size": 200, "burst_duration": 15},
        ]
        
        for config in spike_configs:
            print(f"\næµ‹è¯• {config['name']}: {config['burst_size']}å¹¶å‘, æŒç»­{config['burst_duration']}ç§’")
            
            start_time = time.time()
            
            # åˆ›å»ºå¤§é‡å¹¶å‘è¯·æ±‚
            async def burst_request():
                method, endpoint = random.choice([
                    ("GET", "/api/system/status"),
                    ("GET", "/api/logs?limit=10"),
                ])
                return await self.make_request(method, endpoint)
            
            tasks = [burst_request() for _ in range(config['burst_size'])]
            burst_results = await asyncio.gather(*tasks)
            
            burst_time = time.time() - start_time
            success_count = sum(1 for r in burst_results if r["status"] < 400)
            success_rate = success_count / len(burst_results) * 100
            
            response_times = [r["response_time"] for r in burst_results]
            avg_response = sum(response_times) / len(response_times) * 1000
            max_response = max(response_times) * 1000
            
            print(f"  å®Œæˆ: {success_count}/{config['burst_size']} æˆåŠŸ")
            print(f"  è€—æ—¶: {burst_time:.2f}ç§’")
            print(f"  å¹³å‡å“åº”: {avg_response:.2f}ms")
            print(f"  æœ€å¤§å“åº”: {max_response:.2f}ms")
            
            results.append({
                "spike_type": config['name'],
                "burst_size": config['burst_size'],
                "success_count": success_count,
                "success_rate": success_rate,
                "total_time": burst_time,
                "avg_response_ms": avg_response,
                "max_response_ms": max_response
            })
            
            # ç­‰å¾…ç³»ç»Ÿæ¢å¤
            await asyncio.sleep(5)
        
        return {
            "test_name": "å³°å€¼è´Ÿè½½æµ‹è¯•",
            "results": results
        }
    
    async def test_concurrent_operations(self) -> Dict[str, Any]:
        """æµ‹è¯•4: å¹¶å‘æ“ä½œæµ‹è¯• - å¤šç”¨æˆ·åŒæ—¶æ“ä½œ"""
        print(f"\n{'='*100}")
        print("æµ‹è¯•4: å¹¶å‘æ“ä½œæµ‹è¯•")
        print(f"{'='*100}")
        
        results = []
        
        for concurrent in self.config["concurrent_users"]:
            print(f"\næµ‹è¯• {concurrent} å¹¶å‘ç”¨æˆ·...", end=" ")
            
            # æ¨¡æ‹Ÿä¸åŒç”¨æˆ·çš„ä¸åŒæ“ä½œ
            async def user_operation(user_id: int):
                operations = [
                    ("GET", "/api/accounts"),
                    ("GET", "/api/bots"),
                    ("GET", "/api/mappings"),
                    ("GET", f"/api/logs?limit=10&offset={user_id*10}"),
                    ("GET", "/api/system/stats"),
                ]
                
                user_results = []
                for method, endpoint in operations:
                    result = await self.make_request(method, endpoint)
                    user_results.append(result)
                
                return user_results
            
            start_time = time.time()
            tasks = [user_operation(i) for i in range(concurrent)]
            all_results = await asyncio.gather(*tasks)
            
            total_time = time.time() - start_time
            
            # ç»Ÿè®¡
            all_requests = [r for user in all_results for r in user]
            success = sum(1 for r in all_requests if r["status"] < 400)
            total = len(all_requests)
            success_rate = success / total * 100
            
            qps = total / total_time
            
            print(f"âœ… æˆåŠŸç‡: {success_rate:.1f}%, QPS: {qps:.1f}")
            
            results.append({
                "concurrent_users": concurrent,
                "total_requests": total,
                "success_count": success,
                "success_rate": success_rate,
                "total_time": total_time,
                "qps": qps
            })
        
        return {
            "test_name": "å¹¶å‘æ“ä½œæµ‹è¯•",
            "results": results
        }
    
    async def test_database_stress(self) -> Dict[str, Any]:
        """æµ‹è¯•5: æ•°æ®åº“å‹åŠ›æµ‹è¯•"""
        print(f"\n{'='*100}")
        print("æµ‹è¯•5: æ•°æ®åº“å‹åŠ›æµ‹è¯•")
        print(f"{'='*100}")
        
        db_path = Path(__file__).parent / "backend" / "data" / "kook_forwarder.db"
        
        if not db_path.exists():
            print("âŒ æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨")
            return {"status": "skipped", "reason": "æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨"}
        
        results = []
        
        # æµ‹è¯•å¤§æ‰¹é‡æ’å…¥
        print("\næµ‹è¯•å¤§æ‰¹é‡æ’å…¥ (10000æ¡)...", end=" ")
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        start = time.time()
        cursor.execute("BEGIN TRANSACTION")
        for i in range(10000):
            cursor.execute("""
                INSERT INTO message_logs 
                (kook_message_id, kook_channel_id, content, message_type, sender_name, 
                 target_platform, target_channel, status, latency_ms)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                f"stress_test_{int(time.time())}_{i}",
                "stress_test_channel",
                f"å‹åŠ›æµ‹è¯•æ¶ˆæ¯ {i}",
                "text",
                "stress_test_user",
                "discord",
                "test_target",
                "success",
                random.randint(50, 500)
            ))
        cursor.execute("COMMIT")
        insert_time = time.time() - start
        print(f"âœ… è€—æ—¶: {insert_time:.3f}s, é€Ÿåº¦: {10000/insert_time:.0f} æ¡/ç§’")
        
        results.append({
            "operation": "æ‰¹é‡æ’å…¥",
            "records": 10000,
            "time": insert_time,
            "records_per_second": 10000 / insert_time
        })
        
        # æµ‹è¯•é«˜å¹¶å‘æŸ¥è¯¢
        print("æµ‹è¯•é«˜å¹¶å‘æŸ¥è¯¢ (5000æ¬¡)...", end=" ")
        
        def query_db():
            c = sqlite3.connect(db_path)
            cur = c.cursor()
            cur.execute("""
                SELECT * FROM message_logs 
                WHERE status = 'success' 
                ORDER BY created_at DESC 
                LIMIT 10
            """)
            cur.fetchall()
            c.close()
        
        start = time.time()
        with ThreadPoolExecutor(max_workers=20) as executor:
            futures = [executor.submit(query_db) for _ in range(5000)]
            for future in futures:
                future.result()
        query_time = time.time() - start
        print(f"âœ… è€—æ—¶: {query_time:.3f}s, QPS: {5000/query_time:.0f}")
        
        results.append({
            "operation": "å¹¶å‘æŸ¥è¯¢",
            "queries": 5000,
            "time": query_time,
            "qps": 5000 / query_time
        })
        
        # æ¸…ç†æµ‹è¯•æ•°æ®
        cursor.execute("DELETE FROM message_logs WHERE kook_channel_id = 'stress_test_channel'")
        conn.commit()
        conn.close()
        
        return {
            "test_name": "æ•°æ®åº“å‹åŠ›æµ‹è¯•",
            "status": "success",
            "results": results
        }
    
    async def test_redis_queue_stress(self) -> Dict[str, Any]:
        """æµ‹è¯•6: Redisé˜Ÿåˆ—å‹åŠ›æµ‹è¯•"""
        print(f"\n{'='*100}")
        print("æµ‹è¯•6: Redisé˜Ÿåˆ—å‹åŠ›æµ‹è¯•")
        print(f"{'='*100}")
        
        try:
            import redis
            r = redis.Redis(
                host=self.config["redis_host"],
                port=self.config["redis_port"],
                decode_responses=True
            )
            
            results = []
            
            # æµ‹è¯•é«˜é€Ÿå…¥é˜Ÿ
            print("\næµ‹è¯•é«˜é€Ÿå…¥é˜Ÿ (50000æ¡)...", end=" ")
            start = time.time()
            pipe = r.pipeline()
            for i in range(50000):
                message = json.dumps({
                    "id": f"stress_{i}",
                    "content": f"æ¶ˆæ¯ {i}",
                    "timestamp": time.time()
                })
                pipe.lpush("stress_test_queue", message)
            pipe.execute()
            enqueue_time = time.time() - start
            print(f"âœ… è€—æ—¶: {enqueue_time:.3f}s, é€Ÿåº¦: {50000/enqueue_time:.0f} æ¡/ç§’")
            
            results.append({
                "operation": "é«˜é€Ÿå…¥é˜Ÿ",
                "messages": 50000,
                "time": enqueue_time,
                "messages_per_second": 50000 / enqueue_time
            })
            
            # æµ‹è¯•é«˜é€Ÿå‡ºé˜Ÿ
            print("æµ‹è¯•é«˜é€Ÿå‡ºé˜Ÿ (50000æ¡)...", end=" ")
            start = time.time()
            pipe = r.pipeline()
            for _ in range(50000):
                pipe.rpop("stress_test_queue")
            pipe.execute()
            dequeue_time = time.time() - start
            print(f"âœ… è€—æ—¶: {dequeue_time:.3f}s, é€Ÿåº¦: {50000/dequeue_time:.0f} æ¡/ç§’")
            
            results.append({
                "operation": "é«˜é€Ÿå‡ºé˜Ÿ",
                "messages": 50000,
                "time": dequeue_time,
                "messages_per_second": 50000 / dequeue_time
            })
            
            # æµ‹è¯•å¹¶å‘è¯»å†™
            print("æµ‹è¯•å¹¶å‘è¯»å†™...", end=" ")
            
            async def concurrent_queue_ops():
                r_conn = redis.Redis(
                    host=self.config["redis_host"],
                    port=self.config["redis_port"],
                    decode_responses=True
                )
                
                # å…¥é˜Ÿ
                for i in range(100):
                    r_conn.lpush("concurrent_queue", f"msg_{i}")
                
                # å‡ºé˜Ÿ
                for _ in range(100):
                    r_conn.rpop("concurrent_queue")
            
            start = time.time()
            tasks = [asyncio.to_thread(concurrent_queue_ops) for _ in range(50)]
            await asyncio.gather(*tasks)
            concurrent_time = time.time() - start
            total_ops = 50 * 200  # 50ä¸ªåç¨‹ï¼Œæ¯ä¸ª200æ¬¡æ“ä½œ
            print(f"âœ… {total_ops}æ¬¡æ“ä½œè€—æ—¶: {concurrent_time:.3f}s")
            
            results.append({
                "operation": "å¹¶å‘è¯»å†™",
                "total_operations": total_ops,
                "time": concurrent_time,
                "ops_per_second": total_ops / concurrent_time
            })
            
            return {
                "test_name": "Redisé˜Ÿåˆ—å‹åŠ›æµ‹è¯•",
                "status": "success",
                "results": results
            }
            
        except Exception as e:
            print(f"âŒ Rediså‹åŠ›æµ‹è¯•å¤±è´¥: {e}")
            return {"status": "failed", "error": str(e)}
    
    async def test_memory_leak(self) -> Dict[str, Any]:
        """æµ‹è¯•7: å†…å­˜æ³„æ¼æµ‹è¯•"""
        print(f"\n{'='*100}")
        print("æµ‹è¯•7: å†…å­˜æ³„æ¼æµ‹è¯• (10000æ¬¡è¯·æ±‚)")
        print(f"{'='*100}")
        
        try:
            import psutil
            import gc
            
            # è·å–å½“å‰è¿›ç¨‹
            process = psutil.Process()
            
            # è®°å½•åˆå§‹å†…å­˜
            gc.collect()
            initial_memory = process.memory_info().rss / 1024 / 1024  # MB
            print(f"åˆå§‹å†…å­˜: {initial_memory:.2f} MB")
            
            # æ‰§è¡Œå¤§é‡è¯·æ±‚
            for i in range(0, 10000, 1000):
                tasks = []
                for _ in range(1000):
                    tasks.append(self.make_request("GET", "/api/system/status"))
                await asyncio.gather(*tasks)
                
                # æ¯1000æ¬¡è¯·æ±‚æ£€æŸ¥ä¸€æ¬¡å†…å­˜
                gc.collect()
                current_memory = process.memory_info().rss / 1024 / 1024
                print(f"  {i+1000}æ¬¡è¯·æ±‚å: {current_memory:.2f} MB (å¢é•¿: {current_memory-initial_memory:.2f} MB)")
            
            # æœ€ç»ˆå†…å­˜
            gc.collect()
            final_memory = process.memory_info().rss / 1024 / 1024
            memory_increase = final_memory - initial_memory
            
            print(f"\næœ€ç»ˆå†…å­˜: {final_memory:.2f} MB")
            print(f"æ€»å¢é•¿: {memory_increase:.2f} MB")
            print(f"æ¯è¯·æ±‚: {memory_increase/10000*1024:.2f} KB")
            
            # åˆ¤æ–­æ˜¯å¦æœ‰æ˜æ˜¾å†…å­˜æ³„æ¼
            leak_detected = memory_increase > 100  # å¦‚æœå¢é•¿è¶…è¿‡100MBåˆ™å¯èƒ½æœ‰æ³„æ¼
            
            return {
                "test_name": "å†…å­˜æ³„æ¼æµ‹è¯•",
                "status": "success",
                "initial_memory_mb": initial_memory,
                "final_memory_mb": final_memory,
                "memory_increase_mb": memory_increase,
                "memory_per_request_kb": memory_increase/10000*1024,
                "leak_detected": leak_detected
            }
            
        except Exception as e:
            print(f"âŒ å†…å­˜æµ‹è¯•å¤±è´¥: {e}")
            return {"status": "failed", "error": str(e)}
    
    def generate_comprehensive_report(self, test_results: Dict[str, Any]):
        """ç”Ÿæˆå…¨é¢çš„æµ‹è¯•æŠ¥å‘Š"""
        report_path = Path(__file__).parent / "comprehensive_stress_test_report.md"
        
        with open(report_path, "w", encoding="utf-8") as f:
            f.write("# KOOKæ¶ˆæ¯è½¬å‘ç³»ç»Ÿ - å…¨é¢å‹åŠ›æµ‹è¯•æŠ¥å‘Š\n\n")
            f.write(f"**æµ‹è¯•æ—¶é—´**: {test_results['start_time']}\n")
            f.write(f"**æµ‹è¯•ç¯å¢ƒ**: {self.api_base}\n\n")
            f.write("---\n\n")
            
            f.write("## ğŸ“Š æµ‹è¯•æ€»è§ˆ\n\n")
            f.write("| æµ‹è¯•é¡¹ | çŠ¶æ€ | å…³é”®æŒ‡æ ‡ |\n")
            f.write("|--------|------|----------|\n")
            
            for test_name, test_data in test_results.get("tests", {}).items():
                status = "âœ…" if test_data.get("status") != "failed" else "âŒ"
                
                # æå–å…³é”®æŒ‡æ ‡
                key_metric = ""
                if "qps" in str(test_data):
                    if isinstance(test_data.get("actual_qps"), (int, float)):
                        key_metric = f"QPS: {test_data['actual_qps']:.1f}"
                elif "success_rate" in str(test_data):
                    if isinstance(test_data.get("overall_success_rate"), (int, float)):
                        key_metric = f"æˆåŠŸç‡: {test_data['overall_success_rate']:.1f}%"
                
                f.write(f"| {test_data.get('test_name', test_name)} | {status} | {key_metric} |\n")
            
            f.write("\n---\n\n")
            
            # æ€§èƒ½æ±‡æ€»
            f.write("## ğŸš€ æ€§èƒ½æ±‡æ€»\n\n")
            f.write(f"- **æ€»è¯·æ±‚æ•°**: {self.total_requests:,}\n")
            f.write(f"- **æˆåŠŸè¯·æ±‚**: {self.successful_requests:,}\n")
            f.write(f"- **å¤±è´¥è¯·æ±‚**: {self.failed_requests:,}\n")
            f.write(f"- **æˆåŠŸç‡**: {self.successful_requests/self.total_requests*100:.2f}%\n\n")
            
            if self.response_times:
                sorted_times = sorted(self.response_times)
                f.write("### å“åº”æ—¶é—´ç»Ÿè®¡\n\n")
                f.write(f"- **å¹³å‡**: {sum(self.response_times)/len(self.response_times)*1000:.2f}ms\n")
                f.write(f"- **P50**: {sorted_times[len(sorted_times)//2]*1000:.2f}ms\n")
                f.write(f"- **P90**: {sorted_times[int(len(sorted_times)*0.9)]*1000:.2f}ms\n")
                f.write(f"- **P99**: {sorted_times[int(len(sorted_times)*0.99)]*1000:.2f}ms\n")
                f.write(f"- **æœ€å¤§**: {max(self.response_times)*1000:.2f}ms\n")
                f.write(f"- **æœ€å°**: {min(self.response_times)*1000:.2f}ms\n\n")
            
            f.write("---\n\n")
            
            # è¯¦ç»†æµ‹è¯•ç»“æœ
            for test_name, test_data in test_results.get("tests", {}).items():
                f.write(f"## {test_data.get('test_name', test_name)}\n\n")
                
                if test_data.get("status") == "failed":
                    f.write(f"**çŠ¶æ€**: âŒ å¤±è´¥\n\n")
                    f.write(f"**é”™è¯¯**: {test_data.get('error', 'Unknown')}\n\n")
                    continue
                
                # æ ¹æ®æµ‹è¯•ç±»å‹ç”Ÿæˆä¸åŒçš„æŠ¥å‘Šå†…å®¹
                if "results" in test_data:
                    if isinstance(test_data["results"], list) and test_data["results"]:
                        # è¡¨æ ¼æ ¼å¼
                        keys = test_data["results"][0].keys()
                        f.write("| " + " | ".join(str(k) for k in keys) + " |\n")
                        f.write("|" + "|".join(["---"] * len(keys)) + "|\n")
                        
                        for result in test_data["results"]:
                            values = [str(result.get(k, "")) for k in keys]
                            # æ ¼å¼åŒ–æ•°å­—
                            formatted_values = []
                            for v in values:
                                try:
                                    if '.' in v:
                                        formatted_values.append(f"{float(v):.2f}")
                                    else:
                                        formatted_values.append(v)
                                except:
                                    formatted_values.append(v)
                            f.write("| " + " | ".join(formatted_values) + " |\n")
                    elif isinstance(test_data["results"], dict):
                        for key, value in test_data["results"].items():
                            if isinstance(value, (int, float)):
                                f.write(f"- **{key}**: {value:.2f}\n")
                            else:
                                f.write(f"- **{key}**: {value}\n")
                
                f.write("\n")
            
            f.write("---\n\n")
            f.write(f"**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"**æµ‹è¯•ç”¨ä¾‹**: {len(test_results.get('tests', {}))}ä¸ª\n")
            f.write(f"**æ€»æµ‹è¯•æ—¶é•¿**: {test_results.get('end_time', '')} - {test_results.get('start_time', '')}\n")
        
        print(f"\n\n{'='*100}")
        print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")
        print(f"{'='*100}")
    
    async def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰å‹åŠ›æµ‹è¯•"""
        test_results = {
            "start_time": datetime.now().isoformat(),
            "tests": {}
        }
        
        if not await self.setup():
            return
        
        try:
            # æµ‹è¯•1-4: è´Ÿè½½å’Œååé‡æµ‹è¯•
            for qps in [10, 50, 100]:
                result = await self.test_throughput_under_load(qps)
                test_results["tests"][f"throughput_{qps}qps"] = result
            
            # æµ‹è¯•2: æŒç»­è´Ÿè½½
            result = await self.test_sustained_load()
            test_results["tests"]["sustained_load"] = result
            
            # æµ‹è¯•3: å³°å€¼è´Ÿè½½
            result = await self.test_spike_load()
            test_results["tests"]["spike_load"] = result
            
            # æµ‹è¯•4: å¹¶å‘æ“ä½œ
            result = await self.test_concurrent_operations()
            test_results["tests"]["concurrent_operations"] = result
            
            # æµ‹è¯•5: æ•°æ®åº“å‹åŠ›
            result = await self.test_database_stress()
            test_results["tests"]["database_stress"] = result
            
            # æµ‹è¯•6: Redisé˜Ÿåˆ—å‹åŠ›
            result = await self.test_redis_queue_stress()
            test_results["tests"]["redis_queue_stress"] = result
            
            # æµ‹è¯•7: å†…å­˜æ³„æ¼
            result = await self.test_memory_leak()
            test_results["tests"]["memory_leak"] = result
            
            # ç”ŸæˆæŠ¥å‘Š
            test_results["end_time"] = datetime.now().isoformat()
            self.generate_comprehensive_report(test_results)
            
            # ä¿å­˜JSONç»“æœ
            json_path = Path(__file__).parent / "comprehensive_stress_test_results.json"
            with open(json_path, "w", encoding="utf-8") as f:
                json.dump(test_results, f, indent=2, ensure_ascii=False)
            
            print(f"ğŸ“Š JSONç»“æœå·²ä¿å­˜è‡³: {json_path}")
            
        except Exception as e:
            print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
        
        finally:
            await self.teardown()


async def main():
    """ä¸»å‡½æ•°"""
    print("\n")
    print("â”Œ" + "â”€" * 98 + "â”")
    print("â”‚" + " " * 25 + "KOOKæ¶ˆæ¯è½¬å‘ç³»ç»Ÿ - å…¨é¢å‹åŠ›æµ‹è¯•" + " " * 42 + "â”‚")
    print("â”‚" + " " * 98 + "â”‚")
    print("â”‚  æœ¬æµ‹è¯•å°†å¯¹ç³»ç»Ÿè¿›è¡Œå…¨é¢çš„å‹åŠ›æµ‹è¯•ï¼ŒåŒ…æ‹¬ï¼š" + " " * 49 + "â”‚")
    print("â”‚  â€¢ ååé‡æµ‹è¯•ï¼ˆä¸åŒQPSï¼‰" + " " * 68 + "â”‚")
    print("â”‚  â€¢ æŒç»­è´Ÿè½½æµ‹è¯•ï¼ˆ5åˆ†é’Ÿï¼‰" + " " * 68 + "â”‚")
    print("â”‚  â€¢ å³°å€¼è´Ÿè½½æµ‹è¯•ï¼ˆçªå‘æµé‡ï¼‰" + " " * 64 + "â”‚")
    print("â”‚  â€¢ å¹¶å‘æ“ä½œæµ‹è¯•" + " " * 79 + "â”‚")
    print("â”‚  â€¢ æ•°æ®åº“å‹åŠ›æµ‹è¯•" + " " * 77 + "â”‚")
    print("â”‚  â€¢ Redisé˜Ÿåˆ—å‹åŠ›æµ‹è¯•" + " " * 72 + "â”‚")
    print("â”‚  â€¢ å†…å­˜æ³„æ¼æµ‹è¯•" + " " * 80 + "â”‚")
    print("â”‚" + " " * 98 + "â”‚")
    print("â”‚  é¢„è®¡æ€»è€—æ—¶: 15-20åˆ†é’Ÿ" + " " * 70 + "â”‚")
    print("â””" + "â”€" * 98 + "â”˜")
    print("\n")
    
    runner = ComprehensiveStressTest(COMPREHENSIVE_TEST_CONFIG)
    await runner.run_all_tests()
    
    print("\nâœ… å…¨é¢å‹åŠ›æµ‹è¯•å®Œæˆï¼\n")


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nâš ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(0)
