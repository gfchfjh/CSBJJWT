"""
KOOKæ¶ˆæ¯æŠ“å–å™¨ - æ·±åº¦ä¼˜åŒ–ç‰ˆ
âœ… å®ç°å¯é çš„å¿ƒè·³æ£€æµ‹
âœ… æŒ‡æ•°é€€é¿é‡è¿ç­–ç•¥
âœ… è¿æ¥è´¨é‡ç›‘æ§
âœ… çŠ¶æ€å®æ—¶æ¨é€åˆ°å‰ç«¯
"""
from playwright.async_api import async_playwright, Browser, Page, BrowserContext, WebSocket as PWWebSocket
import asyncio
import json
import time
from typing import Dict, List, Optional, Callable
from pathlib import Path
from ..config import settings
from ..utils.logger import logger
from ..database import db
from ..queue.redis_client import redis_queue


class ConnectionQualityMonitor:
    """è¿æ¥è´¨é‡ç›‘æ§å™¨"""
    
    def __init__(self):
        self.last_message_time = time.time()
        self.message_count = 0
        self.reconnect_count = 0
        self.latency_samples = []
        self.max_samples = 100
        
    def record_message(self):
        """è®°å½•æ”¶åˆ°æ¶ˆæ¯"""
        self.last_message_time = time.time()
        self.message_count += 1
        
    def record_latency(self, latency: float):
        """è®°å½•å»¶è¿Ÿ"""
        self.latency_samples.append(latency)
        if len(self.latency_samples) > self.max_samples:
            self.latency_samples.pop(0)
    
    def record_reconnect(self):
        """è®°å½•é‡è¿"""
        self.reconnect_count += 1
        
    def get_quality_score(self) -> float:
        """
        è®¡ç®—è¿æ¥è´¨é‡è¯„åˆ† (0-100)
        
        è€ƒè™‘å› ç´ ï¼š
        - æ¶ˆæ¯æ¥æ”¶é¢‘ç‡
        - å¹³å‡å»¶è¿Ÿ
        - é‡è¿æ¬¡æ•°
        """
        score = 100.0
        
        # 1. æ£€æŸ¥æ¶ˆæ¯æ–°é²œåº¦ï¼ˆæœ€è¿‘60ç§’å†…æœ‰æ¶ˆæ¯ï¼Ÿï¼‰
        time_since_last_message = time.time() - self.last_message_time
        if time_since_last_message > 300:  # 5åˆ†é’Ÿæ— æ¶ˆæ¯
            score -= 50
        elif time_since_last_message > 120:  # 2åˆ†é’Ÿæ— æ¶ˆæ¯
            score -= 20
            
        # 2. æ£€æŸ¥å¹³å‡å»¶è¿Ÿ
        if self.latency_samples:
            avg_latency = sum(self.latency_samples) / len(self.latency_samples)
            if avg_latency > 5.0:
                score -= 30
            elif avg_latency > 2.0:
                score -= 15
                
        # 3. æƒ©ç½šé¢‘ç¹é‡è¿
        if self.reconnect_count > 5:
            score -= 20
        elif self.reconnect_count > 2:
            score -= 10
            
        return max(0, min(100, score))
    
    def get_status(self) -> dict:
        """è·å–çŠ¶æ€ä¿¡æ¯"""
        return {
            'quality_score': self.get_quality_score(),
            'message_count': self.message_count,
            'last_message_ago': time.time() - self.last_message_time,
            'reconnect_count': self.reconnect_count,
            'avg_latency': sum(self.latency_samples) / len(self.latency_samples) if self.latency_samples else 0
        }


class KookScraperOptimized:
    """KOOKæ¶ˆæ¯æŠ“å–å™¨ - æ·±åº¦ä¼˜åŒ–ç‰ˆ"""
    
    def __init__(self, account_id: int):
        self.account_id = account_id
        self.browser: Optional[Browser] = None
        self.context: Optional[BrowserContext] = None
        self.page: Optional[Page] = None
        self.websocket: Optional[PWWebSocket] = None
        
        self.is_running = False
        self.reconnect_count = 0
        self.max_reconnect = 10  # å¢åŠ åˆ°10æ¬¡
        self.base_reconnect_delay = 5  # åŸºç¡€é‡è¿å»¶è¿Ÿï¼ˆç§’ï¼‰
        
        self.message_handlers: List[Callable] = []
        self.quality_monitor = ConnectionQualityMonitor()
        
        # å¿ƒè·³ç›¸å…³
        self.last_heartbeat = time.time()
        self.heartbeat_interval = 30  # 30ç§’å‘é€ä¸€æ¬¡å¿ƒè·³
        self.heartbeat_timeout = 90  # 90ç§’æ— å“åº”åˆ¤å®šä¸ºæ–­çº¿
        self.heartbeat_task = None
        
        # çŠ¶æ€æ¨é€ç›¸å…³
        self.status_broadcast_interval = 10  # æ¯10ç§’æ¨é€ä¸€æ¬¡çŠ¶æ€
        self.status_task = None
        
    async def start(self):
        """å¯åŠ¨æŠ“å–å™¨"""
        try:
            logger.info(f"[Scraper-{self.account_id}] ğŸš€ æ­£åœ¨å¯åŠ¨ï¼ˆæ·±åº¦ä¼˜åŒ–ç‰ˆï¼‰...")
            
            async with async_playwright() as p:
                # å¯åŠ¨æµè§ˆå™¨
                self.browser = await p.chromium.launch(
                    headless=True,
                    args=[
                        '--no-sandbox',
                        '--disable-setuid-sandbox',
                        '--disable-dev-shm-usage',
                        '--disable-blink-features=AutomationControlled',
                        '--disable-web-security',  # é¿å…CORSé—®é¢˜
                        '--disable-features=IsolateOrigins,site-per-process'
                    ]
                )
                
                # åˆ›å»ºæµè§ˆå™¨ä¸Šä¸‹æ–‡
                self.context = await self.browser.new_context(
                    viewport={'width': 1920, 'height': 1080},
                    user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    java_script_enabled=True
                )
                
                # åŠ è½½Cookie
                cookies = self.load_cookies()
                if cookies:
                    await self.context.add_cookies(cookies)
                    logger.info(f"[Scraper-{self.account_id}] âœ… å·²åŠ è½½Cookie")
                
                # åˆ›å»ºé¡µé¢
                self.page = await self.context.new_page()
                
                # ç›‘å¬WebSocket
                self.page.on('websocket', self.handle_websocket)
                
                # ç›‘å¬æ§åˆ¶å°ï¼ˆç”¨äºè°ƒè¯•ï¼‰
                if settings.debug:
                    self.page.on('console', lambda msg: 
                        logger.debug(f"[Browser Console] {msg.text}")
                    )
                
                # è®¿é—®KOOK
                logger.info(f"[Scraper-{self.account_id}] ğŸŒ æ­£åœ¨è®¿é—®KOOK...")
                await self.broadcast_status('connecting', 'æ­£åœ¨è¿æ¥KOOK...')
                
                response = await self.page.goto(
                    'https://www.kookapp.cn/app', 
                    wait_until='networkidle',
                    timeout=30000
                )
                
                if not response or response.status != 200:
                    raise Exception(f"è®¿é—®KOOKå¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status if response else 'N/A'}")
                
                # ç­‰å¾…é¡µé¢åŠ è½½
                await asyncio.sleep(3)
                
                # æ£€æŸ¥ç™»å½•çŠ¶æ€
                is_logged_in = await self.check_login_status()
                
                if not is_logged_in:
                    logger.warning(f"[Scraper-{self.account_id}] âš ï¸  æœªç™»å½•ï¼Œå¼€å§‹ç™»å½•æµç¨‹...")
                    await self.broadcast_status('login_required', 'éœ€è¦ç™»å½•')
                    
                    # è·å–è´¦å·ä¿¡æ¯
                    account = db.get_account(self.account_id)
                    
                    if not account:
                        raise Exception("è´¦å·ä¸å­˜åœ¨")
                    
                    # å°è¯•ç™»å½•
                    if account.get('password_encrypted'):
                        success = await self.login_with_password(
                            account['email'],
                            self.decrypt_password(account['password_encrypted'])
                        )
                    else:
                        raise Exception("Cookieå·²å¤±æ•ˆï¼Œè¯·é‡æ–°ç™»å½•")
                    
                    if not success:
                        raise Exception("ç™»å½•å¤±è´¥")
                
                logger.info(f"[Scraper-{self.account_id}] âœ… ç™»å½•æˆåŠŸï¼Œå¼€å§‹ç›‘å¬æ¶ˆæ¯...")
                await self.broadcast_status('connected', 'å·²è¿æ¥ï¼Œæ­£åœ¨ç›‘å¬')
                
                # æ›´æ–°è´¦å·çŠ¶æ€
                db.update_account_status(self.account_id, "online")
                
                # å¯åŠ¨å¿ƒè·³ä»»åŠ¡
                self.heartbeat_task = asyncio.create_task(self.heartbeat_loop())
                
                # å¯åŠ¨çŠ¶æ€å¹¿æ’­ä»»åŠ¡
                self.status_task = asyncio.create_task(self.status_broadcast_loop())
                
                # ä¿æŒè¿è¡Œ
                self.is_running = True
                self.reconnect_count = 0  # é‡ç½®é‡è¿è®¡æ•°
                
                while self.is_running:
                    await asyncio.sleep(1)
                    
                    # æ£€æŸ¥å¿ƒè·³è¶…æ—¶
                    if time.time() - self.last_heartbeat > self.heartbeat_timeout:
                        logger.warning(f"[Scraper-{self.account_id}] ğŸ’” å¿ƒè·³è¶…æ—¶ï¼Œè¿æ¥å¯èƒ½å·²æ–­å¼€")
                        await self.broadcast_status('heartbeat_timeout', 'å¿ƒè·³è¶…æ—¶')
                        break
                        
        except Exception as e:
            logger.error(f"[Scraper-{self.account_id}] âŒ å¯åŠ¨å¤±è´¥: {str(e)}")
            await self.broadcast_status('error', f'å¯åŠ¨å¤±è´¥: {str(e)}')
            db.update_account_status(self.account_id, "offline")
            raise
        finally:
            await self.stop()
            
            # å°è¯•é‡è¿
            if self.is_running and self.reconnect_count < self.max_reconnect:
                await self.reconnect()
    
    async def heartbeat_loop(self):
        """å¿ƒè·³å¾ªç¯"""
        while self.is_running:
            try:
                await asyncio.sleep(self.heartbeat_interval)
                
                if self.page and not self.page.is_closed():
                    # å‘é€å¿ƒè·³ï¼ˆæ‰§è¡Œç®€å•çš„JSï¼‰
                    start_time = time.time()
                    result = await self.page.evaluate('() => Date.now()')
                    latency = time.time() - start_time
                    
                    self.last_heartbeat = time.time()
                    self.quality_monitor.record_latency(latency * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
                    
                    if latency > 2.0:
                        logger.warning(f"[Scraper-{self.account_id}] âš ï¸  å¿ƒè·³å»¶è¿Ÿè¿‡é«˜: {latency:.2f}s")
                    else:
                        logger.debug(f"[Scraper-{self.account_id}] ğŸ’“ å¿ƒè·³æ­£å¸¸ (å»¶è¿Ÿ: {latency*1000:.0f}ms)")
                else:
                    logger.warning(f"[Scraper-{self.account_id}] âš ï¸  é¡µé¢å·²å…³é—­ï¼Œåœæ­¢å¿ƒè·³")
                    break
                    
            except Exception as e:
                logger.error(f"[Scraper-{self.account_id}] âŒ å¿ƒè·³å¼‚å¸¸: {str(e)}")
                break
    
    async def status_broadcast_loop(self):
        """çŠ¶æ€å¹¿æ’­å¾ªç¯"""
        while self.is_running:
            try:
                await asyncio.sleep(self.status_broadcast_interval)
                
                # è·å–è¿æ¥è´¨é‡
                quality_status = self.quality_monitor.get_status()
                quality_score = quality_status['quality_score']
                
                # ç¡®å®šçŠ¶æ€
                if quality_score >= 80:
                    status = 'healthy'
                    message = 'è¿æ¥è‰¯å¥½'
                elif quality_score >= 50:
                    status = 'degraded'
                    message = 'è¿æ¥è´¨é‡ä¸‹é™'
                else:
                    status = 'poor'
                    message = 'è¿æ¥è´¨é‡å·®'
                
                await self.broadcast_status(status, message, quality_status)
                
            except Exception as e:
                logger.error(f"[Scraper-{self.account_id}] çŠ¶æ€å¹¿æ’­å¼‚å¸¸: {str(e)}")
    
    async def broadcast_status(self, status: str, message: str, extra: dict = None):
        """
        å¹¿æ’­çŠ¶æ€åˆ°å‰ç«¯
        
        Args:
            status: çŠ¶æ€ç±»å‹
            message: çŠ¶æ€æ¶ˆæ¯
            extra: é¢å¤–ä¿¡æ¯
        """
        try:
            # æ„å»ºçŠ¶æ€æ•°æ®
            status_data = {
                'account_id': self.account_id,
                'status': status,
                'message': message,
                'timestamp': time.time()
            }
            
            if extra:
                status_data.update(extra)
            
            # é€šè¿‡Rediså‘å¸ƒçŠ¶æ€
            await redis_queue.publish('scraper_status', json.dumps(status_data))
            
            logger.debug(f"[Scraper-{self.account_id}] ğŸ“¡ çŠ¶æ€å·²å¹¿æ’­: {status} - {message}")
            
        except Exception as e:
            logger.error(f"[Scraper-{self.account_id}] å¹¿æ’­çŠ¶æ€å¤±è´¥: {str(e)}")
    
    async def check_login_status(self) -> bool:
        """æ£€æŸ¥ç™»å½•çŠ¶æ€"""
        try:
            # æ–¹æ³•1: æ£€æŸ¥ç™»å½•è¡¨å•
            login_form = await self.page.query_selector('form[class*="login"]')
            if login_form:
                return False
            
            # æ–¹æ³•2: æ£€æŸ¥ç”¨æˆ·ä¿¡æ¯å…ƒç´ 
            user_info = await self.page.query_selector('[class*="user-info"], [class*="avatar"]')
            if user_info:
                return True
            
            # æ–¹æ³•3: æ‰§è¡ŒJSæ£€æŸ¥localStorage
            is_logged_in = await self.page.evaluate('''() => {
                const token = window.localStorage.getItem('token') || 
                             window.localStorage.getItem('kaiheila_token') ||
                             window.localStorage.getItem('access_token');
                return token !== null && token !== '';
            }''')
            
            return is_logged_in
            
        except Exception as e:
            logger.debug(f"[Scraper-{self.account_id}] ç™»å½•çŠ¶æ€æ£€æŸ¥å¼‚å¸¸: {e}")
            return False
    
    async def login_with_password(self, email: str, password: str) -> bool:
        """è´¦å·å¯†ç ç™»å½•"""
        try:
            logger.info(f"[Scraper-{self.account_id}] ğŸ” å¼€å§‹è´¦å·å¯†ç ç™»å½•...")
            
            await self.page.wait_for_selector('input[name="email"], input[type="email"]', timeout=10000)
            
            await self.page.fill('input[name="email"], input[type="email"]', email)
            await asyncio.sleep(0.5)
            
            await self.page.fill('input[name="password"], input[type="password"]', password)
            await asyncio.sleep(0.5)
            
            await self.page.click('button[type="submit"]')
            
            try:
                await self.page.wait_for_selector('.app-container, [class*="main"]', timeout=10000)
                logger.info(f"[Scraper-{self.account_id}] âœ… ç™»å½•æˆåŠŸ")
                
                await self.save_cookies()
                return True
                
            except Exception:
                captcha_element = await self.page.query_selector('.captcha-container, [class*="captcha"]')
                if captcha_element:
                    logger.warning(f"[Scraper-{self.account_id}] ğŸ”’ éœ€è¦éªŒè¯ç ")
                    await self.broadcast_status('captcha_required', 'éœ€è¦è¾“å…¥éªŒè¯ç ')
                    success = await self.handle_captcha()
                    if success:
                        await self.save_cookies()
                    return success
                else:
                    return False
                    
        except Exception as e:
            logger.error(f"[Scraper-{self.account_id}] ç™»å½•å¼‚å¸¸: {str(e)}")
            return False
    
    async def handle_captcha(self) -> bool:
        """å¤„ç†éªŒè¯ç ï¼ˆå¾…å®ç°ï¼‰"""
        # TODO: å®ç°éªŒè¯ç å¤„ç†é€»è¾‘
        logger.info(f"[Scraper-{self.account_id}] â³ ç­‰å¾…ç”¨æˆ·è¾“å…¥éªŒè¯ç ...")
        return False
    
    async def handle_websocket(self, ws: PWWebSocket):
        """å¤„ç†WebSocketè¿æ¥"""
        logger.info(f"[Scraper-{self.account_id}] ğŸ”Œ WebSocketè¿æ¥å·²å»ºç«‹: {ws.url}")
        self.websocket = ws
        
        # ç›‘å¬æ¶ˆæ¯
        ws.on('framereceived', lambda payload: 
            asyncio.create_task(self.process_websocket_message(payload))
        )
        
        # ç›‘å¬å…³é—­
        ws.on('close', lambda: 
            logger.warning(f"[Scraper-{self.account_id}] âš ï¸  WebSocketè¿æ¥å·²å…³é—­")
        )
    
    async def process_websocket_message(self, payload: bytes):
        """å¤„ç†WebSocketæ¶ˆæ¯"""
        try:
            data = json.loads(payload.decode('utf-8'))
            
            # è®°å½•æ¶ˆæ¯æ¥æ”¶
            self.quality_monitor.record_message()
            
            msg_type = data.get('type')
            
            if msg_type == 'MESSAGE_CREATE':
                message = self.parse_message(data)
                
                if message:
                    logger.info(
                        f"[Scraper-{self.account_id}] ğŸ“¨ æ”¶åˆ°æ¶ˆæ¯: "
                        f"é¢‘é“={message.get('channel_name', 'Unknown')}, "
                        f"ä½œè€…={message['author']['username']}"
                    )
                    
                    # å…¥é˜Ÿå¤„ç†
                    await redis_queue.enqueue('message_queue', message)
                    
                    # è°ƒç”¨æ¶ˆæ¯å¤„ç†å™¨
                    for handler in self.message_handlers:
                        try:
                            await handler(message)
                        except Exception as e:
                            logger.error(f"æ¶ˆæ¯å¤„ç†å™¨æ‰§è¡Œå¤±è´¥: {e}")
            
        except json.JSONDecodeError:
            pass  # å¿½ç•¥éJSONæ¶ˆæ¯
        except Exception as e:
            logger.error(f"[Scraper-{self.account_id}] å¤„ç†WebSocketæ¶ˆæ¯å¼‚å¸¸: {str(e)}")
    
    def parse_message(self, data: Dict) -> Optional[Dict]:
        """è§£ææ¶ˆæ¯æ•°æ®"""
        try:
            d = data.get('d', {})
            
            message = {
                'account_id': self.account_id,
                'kook_message_id': d.get('msg_id'),
                'channel_id': d.get('target_id'),
                'server_id': d.get('guild_id'),
                'message_type': d.get('type', 1),
                'content': d.get('content', ''),
                'created_at': d.get('msg_timestamp', int(time.time() * 1000)),
            }
            
            # ä½œè€…ä¿¡æ¯
            author = d.get('author', {})
            message['author'] = {
                'id': author.get('id'),
                'username': author.get('username'),
                'nickname': author.get('nickname'),
                'avatar': author.get('avatar'),
                'bot': author.get('bot', False)
            }
            
            # é™„ä»¶
            attachments = d.get('attachments', [])
            if attachments:
                message['attachments'] = attachments
            
            # @æåŠ
            message['mention_all'] = d.get('mention_all', False)
            message['mention_users'] = d.get('mention', [])
            message['mention_roles'] = d.get('mention_roles', [])
            
            # å¼•ç”¨æ¶ˆæ¯
            if 'quote' in d and d['quote']:
                message['quote'] = {
                    'id': d['quote'].get('id'),
                    'content': d['quote'].get('content'),
                    'author': d['quote'].get('author', {})
                }
            
            return message
            
        except Exception as e:
            logger.error(f"[Scraper-{self.account_id}] æ¶ˆæ¯è§£æå¤±è´¥: {str(e)}")
            return None
    
    async def reconnect(self):
        """é‡æ–°è¿æ¥ï¼ˆæŒ‡æ•°é€€é¿ç­–ç•¥ï¼‰"""
        self.reconnect_count += 1
        self.quality_monitor.record_reconnect()
        
        if self.reconnect_count > self.max_reconnect:
            logger.error(f"[Scraper-{self.account_id}] âŒ è¶…è¿‡æœ€å¤§é‡è¿æ¬¡æ•°({self.max_reconnect})ï¼Œåœæ­¢å°è¯•")
            await self.broadcast_status('failed', 'è¶…è¿‡æœ€å¤§é‡è¿æ¬¡æ•°')
            self.is_running = False
            return
        
        # è®¡ç®—é‡è¿å»¶è¿Ÿï¼ˆæŒ‡æ•°é€€é¿ï¼‰
        delay = min(self.base_reconnect_delay * (2 ** (self.reconnect_count - 1)), 300)  # æœ€å¤š5åˆ†é’Ÿ
        
        logger.info(f"[Scraper-{self.account_id}] ğŸ”„ ç¬¬{self.reconnect_count}æ¬¡é‡è¿ï¼ˆ{delay}ç§’åï¼‰...")
        await self.broadcast_status('reconnecting', f'ç¬¬{self.reconnect_count}æ¬¡é‡è¿')
        
        await asyncio.sleep(delay)
        
        try:
            await self.start()
        except Exception as e:
            logger.error(f"[Scraper-{self.account_id}] é‡è¿å¤±è´¥: {str(e)}")
            await self.reconnect()
    
    def load_cookies(self) -> List[Dict]:
        """ä»æ•°æ®åº“åŠ è½½Cookie"""
        try:
            account = db.get_account(self.account_id)
            
            if account and account.get('cookie'):
                cookies = json.loads(account['cookie'])
                return cookies
            
            return []
            
        except Exception as e:
            logger.error(f"[Scraper-{self.account_id}] åŠ è½½Cookieå¤±è´¥: {str(e)}")
            return []
    
    async def save_cookies(self):
        """ä¿å­˜Cookieåˆ°æ•°æ®åº“"""
        try:
            cookies = await self.context.cookies()
            
            db.update_account_cookie(self.account_id, json.dumps(cookies))
            
            logger.info(f"[Scraper-{self.account_id}] âœ… Cookieå·²ä¿å­˜")
            
        except Exception as e:
            logger.error(f"[Scraper-{self.account_id}] ä¿å­˜Cookieå¤±è´¥: {str(e)}")
    
    def decrypt_password(self, encrypted: str) -> str:
        """è§£å¯†å¯†ç """
        from ..utils.crypto import decrypt
        return decrypt(encrypted)
    
    def register_message_handler(self, handler: Callable):
        """æ³¨å†Œæ¶ˆæ¯å¤„ç†å™¨"""
        self.message_handlers.append(handler)
    
    async def stop(self):
        """åœæ­¢æŠ“å–å™¨"""
        logger.info(f"[Scraper-{self.account_id}] ğŸ›‘ æ­£åœ¨åœæ­¢...")
        
        self.is_running = False
        
        try:
            # å–æ¶ˆå¿ƒè·³ä»»åŠ¡
            if self.heartbeat_task:
                self.heartbeat_task.cancel()
                
            # å–æ¶ˆçŠ¶æ€å¹¿æ’­ä»»åŠ¡
            if self.status_task:
                self.status_task.cancel()
            
            if self.page and not self.page.is_closed():
                await self.page.close()
            
            if self.context:
                await self.context.close()
            
            if self.browser:
                await self.browser.close()
            
            db.update_account_status(self.account_id, "offline")
            await self.broadcast_status('stopped', 'å·²åœæ­¢')
            
            logger.info(f"[Scraper-{self.account_id}] âœ… å·²åœæ­¢")
            
        except Exception as e:
            logger.error(f"[Scraper-{self.account_id}] åœæ­¢å¼‚å¸¸: {str(e)}")


# ç»§ç»­ä½¿ç”¨åŸæœ‰çš„ScraperManagerï¼Œä½†ä½¿ç”¨ä¼˜åŒ–ç‰ˆçš„Scraper
class ScraperManagerOptimized:
    """æŠ“å–å™¨ç®¡ç†å™¨ - ä¼˜åŒ–ç‰ˆ"""
    
    def __init__(self):
        self.scrapers: Dict[int, KookScraperOptimized] = {}
        self.tasks: Dict[int, asyncio.Task] = {}
    
    async def start_scraper(self, account_id: int):
        """å¯åŠ¨æŒ‡å®šè´¦å·çš„æŠ“å–å™¨"""
        if account_id in self.scrapers:
            logger.warning(f"è´¦å·{account_id}çš„æŠ“å–å™¨å·²åœ¨è¿è¡Œ")
            return
        
        scraper = KookScraperOptimized(account_id)
        self.scrapers[account_id] = scraper
        
        task = asyncio.create_task(scraper.start())
        self.tasks[account_id] = task
        
        logger.info(f"è´¦å·{account_id}çš„æŠ“å–å™¨å·²å¯åŠ¨ï¼ˆä¼˜åŒ–ç‰ˆï¼‰")
    
    async def stop_scraper(self, account_id: int):
        """åœæ­¢æŒ‡å®šè´¦å·çš„æŠ“å–å™¨"""
        if account_id not in self.scrapers:
            logger.warning(f"è´¦å·{account_id}çš„æŠ“å–å™¨æœªè¿è¡Œ")
            return
        
        scraper = self.scrapers[account_id]
        await scraper.stop()
        
        if account_id in self.tasks:
            self.tasks[account_id].cancel()
            try:
                await self.tasks[account_id]
            except asyncio.CancelledError:
                pass
            del self.tasks[account_id]
        
        del self.scrapers[account_id]
        
        logger.info(f"è´¦å·{account_id}çš„æŠ“å–å™¨å·²åœæ­¢")
    
    async def start_all(self):
        """å¯åŠ¨æ‰€æœ‰è´¦å·çš„æŠ“å–å™¨"""
        accounts = db.get_accounts()
        
        for account in accounts:
            try:
                await self.start_scraper(account['id'])
            except Exception as e:
                logger.error(f"å¯åŠ¨è´¦å·{account['id']}çš„æŠ“å–å™¨å¤±è´¥: {e}")
    
    async def stop_all(self):
        """åœæ­¢æ‰€æœ‰æŠ“å–å™¨"""
        account_ids = list(self.scrapers.keys())
        
        for account_id in account_ids:
            try:
                await self.stop_scraper(account_id)
            except Exception as e:
                logger.error(f"åœæ­¢è´¦å·{account_id}çš„æŠ“å–å™¨å¤±è´¥: {e}")
    
    def get_scraper(self, account_id: int) -> Optional[KookScraperOptimized]:
        """è·å–æŒ‡å®šè´¦å·çš„æŠ“å–å™¨"""
        return self.scrapers.get(account_id)
    
    def get_all_scrapers(self) -> Dict[int, KookScraperOptimized]:
        """è·å–æ‰€æœ‰æŠ“å–å™¨"""
        return self.scrapers
    
    def get_status(self) -> dict:
        """è·å–æ‰€æœ‰æŠ“å–å™¨çš„çŠ¶æ€"""
        return {
            'total': len(self.scrapers),
            'running': sum(1 for s in self.scrapers.values() if s.is_running),
            'scrapers': {
                account_id: {
                    'is_running': scraper.is_running,
                    'reconnect_count': scraper.reconnect_count,
                    'quality': scraper.quality_monitor.get_status()
                }
                for account_id, scraper in self.scrapers.items()
            }
        }


# å…¨å±€æŠ“å–å™¨ç®¡ç†å™¨
scraper_manager_optimized = ScraperManagerOptimized()
