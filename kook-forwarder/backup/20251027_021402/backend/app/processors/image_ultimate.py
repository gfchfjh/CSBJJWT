"""
å›¾ç‰‡å¤„ç†å™¨ï¼ˆç»ˆæä¼˜åŒ–ç‰ˆï¼‰
======================
åŠŸèƒ½ï¼š
1. å¹¶å‘ä¸‹è½½ï¼ˆaiohttpå¹¶å‘ï¼‰
2. å¤šè¿›ç¨‹å‹ç¼©ï¼ˆCPUå¯†é›†ï¼‰
3. æ™ºèƒ½ç¼“å­˜
4. é˜²ç›—é“¾å¤„ç†
5. è‡ªåŠ¨é‡è¯•

ä½œè€…ï¼šKOOK Forwarder Team
æ—¥æœŸï¼š2025-10-25
"""

import asyncio
import aiohttp
from concurrent.futures import ProcessPoolExecutor
from typing import List, Dict, Optional, Tuple
from pathlib import Path
from PIL import Image
import io
import hashlib
from ..utils.logger import logger
from ..config import settings


class ImageProcessorUltimate:
    """å›¾ç‰‡å¤„ç†å™¨ï¼ˆç»ˆæä¼˜åŒ–ç‰ˆï¼‰"""
    
    def __init__(self):
        # åˆ›å»ºå¤šè¿›ç¨‹æ± ï¼ˆç”¨äºå›¾ç‰‡å‹ç¼©ï¼‰
        self.process_pool = ProcessPoolExecutor(max_workers=4)
        
        # åˆ›å»ºaiohttpä¼šè¯ï¼ˆç”¨äºå¹¶å‘ä¸‹è½½ï¼‰
        self.session: Optional[aiohttp.ClientSession] = None
        
        # ä¸‹è½½ç¼“å­˜ï¼ˆé¿å…é‡å¤ä¸‹è½½ï¼‰
        self.download_cache: Dict[str, bytes] = {}
        self.max_cache_size = 100  # æœ€å¤šç¼“å­˜100å¼ å›¾ç‰‡
    
    async def ensure_session(self):
        """ç¡®ä¿aiohttpä¼šè¯å­˜åœ¨"""
        if not self.session or self.session.closed:
            self.session = aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=30),
                connector=aiohttp.TCPConnector(limit=20)  # æœ€å¤š20ä¸ªå¹¶å‘è¿æ¥
            )
    
    async def download_images_concurrent(self, urls: List[str], 
                                        cookies: Dict = None,
                                        referer: str = None) -> List[Tuple[str, Optional[bytes]]]:
        """
        å¹¶å‘ä¸‹è½½å¤šå¼ å›¾ç‰‡ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
        
        Args:
            urls: å›¾ç‰‡URLåˆ—è¡¨
            cookies: Cookieå­—å…¸
            referer: Refererå¤´
            
        Returns:
            [(url, image_data), ...]
        """
        await self.ensure_session()
        
        logger.info(f"ğŸš€ å¹¶å‘ä¸‹è½½ {len(urls)} å¼ å›¾ç‰‡...")
        
        # åˆ›å»ºå¹¶å‘ä¸‹è½½ä»»åŠ¡
        tasks = [
            self._download_single_image(url, cookies, referer)
            for url in urls
        ]
        
        # å¹¶å‘æ‰§è¡Œ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†ç»“æœ
        downloaded = []
        for url, result in zip(urls, results):
            if isinstance(result, Exception):
                logger.error(f"ä¸‹è½½å¤±è´¥: {url} - {result}")
                downloaded.append((url, None))
            else:
                downloaded.append((url, result))
        
        success_count = sum(1 for _, data in downloaded if data)
        logger.info(f"âœ… ä¸‹è½½å®Œæˆ: {success_count}/{len(urls)} æˆåŠŸ")
        
        return downloaded
    
    async def _download_single_image(self, url: str, 
                                    cookies: Dict = None,
                                    referer: str = None) -> Optional[bytes]:
        """
        ä¸‹è½½å•å¼ å›¾ç‰‡ï¼ˆæ”¯æŒç¼“å­˜ï¼‰
        
        Args:
            url: å›¾ç‰‡URL
            cookies: Cookieå­—å…¸
            referer: Refererå¤´
            
        Returns:
            å›¾ç‰‡æ•°æ®
        """
        # æ£€æŸ¥ç¼“å­˜
        cache_key = hashlib.md5(url.encode()).hexdigest()
        if cache_key in self.download_cache:
            logger.debug(f"ä½¿ç”¨ç¼“å­˜å›¾ç‰‡: {url}")
            return self.download_cache[cache_key]
        
        try:
            # å‡†å¤‡è¯·æ±‚å¤´
            headers = {}
            if referer:
                headers['Referer'] = referer
            if cookies:
                # è½¬æ¢Cookieå­—å…¸ä¸ºCookieå­—ç¬¦ä¸²
                cookie_str = '; '.join(f"{k}={v}" for k, v in cookies.items())
                headers['Cookie'] = cookie_str
            
            # ä¸‹è½½å›¾ç‰‡
            async with self.session.get(url, headers=headers) as response:
                if response.status == 200:
                    data = await response.read()
                    
                    # åŠ å…¥ç¼“å­˜
                    if len(self.download_cache) >= self.max_cache_size:
                        # åˆ é™¤æœ€æ—§çš„ç¼“å­˜
                        oldest_key = next(iter(self.download_cache))
                        del self.download_cache[oldest_key]
                    
                    self.download_cache[cache_key] = data
                    
                    logger.debug(f"ä¸‹è½½æˆåŠŸ: {url} ({len(data) / 1024:.1f} KB)")
                    return data
                else:
                    logger.error(f"ä¸‹è½½å¤±è´¥: {url} - HTTP {response.status}")
                    return None
                    
        except Exception as e:
            logger.error(f"ä¸‹è½½å¼‚å¸¸: {url} - {e}")
            return None
    
    @staticmethod
    def _compress_image_worker(image_data: bytes, max_size_mb: float, quality: int) -> bytes:
        """
        å‹ç¼©å›¾ç‰‡ï¼ˆé™æ€æ–¹æ³•ï¼Œç”¨äºå¤šè¿›ç¨‹æ± ï¼‰
        
        Args:
            image_data: åŸå§‹å›¾ç‰‡æ•°æ®
            max_size_mb: æœ€å¤§å¤§å°ï¼ˆMBï¼‰
            quality: å‹ç¼©è´¨é‡ï¼ˆ1-100ï¼‰
            
        Returns:
            å‹ç¼©åçš„å›¾ç‰‡æ•°æ®
        """
        try:
            # æ‰“å¼€å›¾ç‰‡
            img = Image.open(io.BytesIO(image_data))
            
            # è½¬æ¢RGBAåˆ°RGBï¼ˆå¦‚æœéœ€è¦ï¼‰
            if img.mode in ('RGBA', 'LA', 'P'):
                background = Image.new('RGB', img.size, (255, 255, 255))
                background.paste(img, mask=img.split()[-1] if img.mode == 'RGBA' else None)
                img = background
            
            # å‹ç¼©å›¾ç‰‡
            output = io.BytesIO()
            img.save(output, format='JPEG', quality=quality, optimize=True)
            compressed_data = output.getvalue()
            
            # æ£€æŸ¥å¤§å°
            size_mb = len(compressed_data) / (1024 * 1024)
            
            if size_mb > max_size_mb:
                # é€’å½’é™ä½è´¨é‡
                if quality > 50:
                    return ImageProcessorUltimate._compress_image_worker(
                        image_data, max_size_mb, quality - 10
                    )
            
            return compressed_data
            
        except Exception as e:
            # å¤šè¿›ç¨‹ä¸­æ— æ³•ä½¿ç”¨loggerï¼Œè¿”å›åŸæ•°æ®
            return image_data
    
    async def process_images_concurrent(self, image_urls: List[str],
                                       cookies: Dict = None) -> List[Dict]:
        """
        å¹¶å‘å¤„ç†å¤šå¼ å›¾ç‰‡ï¼ˆä¸‹è½½+å‹ç¼©ï¼‰
        
        Args:
            image_urls: å›¾ç‰‡URLåˆ—è¡¨
            cookies: Cookieå­—å…¸
            
        Returns:
            å¤„ç†åçš„å›¾ç‰‡ä¿¡æ¯åˆ—è¡¨
        """
        # æ­¥éª¤1: å¹¶å‘ä¸‹è½½
        downloaded = await self.download_images_concurrent(
            image_urls, 
            cookies=cookies, 
            referer='https://www.kookapp.cn'
        )
        
        # æ­¥éª¤2: å¹¶å‘å‹ç¼©ï¼ˆä½¿ç”¨å¤šè¿›ç¨‹æ± ï¼‰
        compress_tasks = []
        loop = asyncio.get_event_loop()
        
        for url, image_data in downloaded:
            if image_data:
                task = loop.run_in_executor(
                    self.process_pool,
                    self._compress_image_worker,
                    image_data,
                    settings.image_max_size_mb,
                    settings.image_compression_quality
                )
                compress_tasks.append((url, task))
        
        # ç­‰å¾…å‹ç¼©å®Œæˆ
        processed_images = []
        for url, task in compress_tasks:
            try:
                compressed_data = await task
                
                # ä¿å­˜åˆ°æœ¬åœ°
                image_path = await self._save_to_local(url, compressed_data)
                
                processed_images.append({
                    'original_url': url,
                    'local_path': str(image_path),
                    'size': len(compressed_data)
                })
                
            except Exception as e:
                logger.error(f"å›¾ç‰‡å¤„ç†å¤±è´¥: {url} - {e}")
        
        return processed_images
    
    async def _save_to_local(self, url: str, data: bytes) -> Path:
        """ä¿å­˜å›¾ç‰‡åˆ°æœ¬åœ°"""
        # ç”Ÿæˆæ–‡ä»¶å
        url_hash = hashlib.md5(url.encode()).hexdigest()
        filename = f"{url_hash}.jpg"
        filepath = settings.image_storage_path / filename
        
        # å†™å…¥æ–‡ä»¶
        filepath.write_bytes(data)
        
        return filepath
    
    async def close(self):
        """å…³é—­èµ„æº"""
        if self.session and not self.session.closed:
            await self.session.close()
        
        self.process_pool.shutdown(wait=True)


# å…¨å±€å›¾ç‰‡å¤„ç†å™¨å®ä¾‹
image_processor_ultimate = ImageProcessorUltimate()
