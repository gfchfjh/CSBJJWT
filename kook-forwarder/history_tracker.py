"""
å†å²æ•°æ®è·Ÿè¸ªå™¨
è®°å½•å’Œå¯¹æ¯”å†å²æµ‹è¯•æ•°æ®
"""
import json
import os
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any


class HistoryTracker:
    """å†å²æ•°æ®è·Ÿè¸ªå™¨"""
    
    def __init__(self, history_dir="test_results/history"):
        self.history_dir = Path(history_dir)
        self.history_dir.mkdir(parents=True, exist_ok=True)
        self.max_records = 100  # æœ€å¤šä¿ç•™100æ¡å†å²è®°å½•
    
    def save_current_results(self):
        """ä¿å­˜å½“å‰æµ‹è¯•ç»“æœåˆ°å†å²è®°å½•"""
        print("ğŸ“Š ä¿å­˜æµ‹è¯•ç»“æœåˆ°å†å²è®°å½•...")
        
        test_results_dir = Path("test_results")
        if not test_results_dir.exists():
            print("âŒ test_results ç›®å½•ä¸å­˜åœ¨")
            return
        
        # åˆ›å»ºå†å²è®°å½•æ¡ç›®
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        history_entry = {
            "timestamp": datetime.now().isoformat(),
            "test_date": datetime.now().strftime("%Y-%m-%d"),
            "test_time": datetime.now().strftime("%H:%M:%S"),
            "reports": {}
        }
        
        # æ”¶é›†æ‰€æœ‰æµ‹è¯•æŠ¥å‘Š
        report_files = list(test_results_dir.glob("*_report.json"))
        
        for report_file in report_files:
            try:
                with open(report_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # æå–å…³é”®æŒ‡æ ‡
                metrics = self._extract_metrics(data, report_file.stem)
                if metrics:
                    history_entry["reports"][report_file.stem] = metrics
            
            except Exception as e:
                print(f"  âš ï¸  è¯»å– {report_file.name} å¤±è´¥: {e}")
        
        # ä¿å­˜å†å²è®°å½•
        history_file = self.history_dir / f"history_{timestamp}.json"
        
        try:
            with open(history_file, 'w', encoding='utf-8') as f:
                json.dump(history_entry, f, indent=2, ensure_ascii=False)
            
            print(f"  âœ… å†å²è®°å½•å·²ä¿å­˜: {history_file.name}")
            
            # æ¸…ç†æ—§è®°å½•
            self._cleanup_old_records()
            
            return history_file
        
        except Exception as e:
            print(f"  âŒ ä¿å­˜å†å²è®°å½•å¤±è´¥: {e}")
            return None
    
    def _extract_metrics(self, data, report_type):
        """æå–å…³é”®æ€§èƒ½æŒ‡æ ‡"""
        metrics = {}
        
        if 'comprehensive' in report_type and 'test_results' in data:
            test_results = data['test_results']
            
            # APIæŒ‡æ ‡
            if 'api_stress' in test_results and 'results' in test_results['api_stress']:
                api_results = test_results['api_stress']['results']
                if api_results:
                    # å–å¹¶å‘100çš„æ•°æ®
                    for r in api_results:
                        if r.get('concurrent') == 100:
                            metrics['api_qps_100'] = r.get('qps', 0)
                            metrics['api_avg_latency'] = r.get('avg_time_ms', 0)
                            break
            
            # æ ¼å¼è½¬æ¢æŒ‡æ ‡
            if 'formatter_stress' in test_results and 'results' in test_results['formatter_stress']:
                formatter_results = test_results['formatter_stress']['results']
                if formatter_results:
                    last = formatter_results[-1]
                    if 'discord' in last:
                        metrics['discord_ops'] = last['discord'].get('ops_per_sec', 0)
                    if 'telegram' in last:
                        metrics['telegram_ops'] = last['telegram'].get('ops_per_sec', 0)
            
            # Redisé˜Ÿåˆ—æŒ‡æ ‡
            if 'redis_queue_stress' in test_results and 'results' in test_results['redis_queue_stress']:
                queue_results = test_results['redis_queue_stress']['results']
                for r in queue_results:
                    if r.get('batch_size') == 1000:
                        metrics['redis_enqueue_qps'] = r.get('enqueue_qps', 0)
                        metrics['redis_dequeue_qps'] = r.get('dequeue_qps', 0)
                        break
        
        elif 'demo' in report_type and 'results' in data:
            results = data['results']
            
            # æå–æ¼”ç¤ºæµ‹è¯•æŒ‡æ ‡
            if 'message_formatting' in results and results['message_formatting']:
                last = results['message_formatting'][-1]
                metrics['format_ops'] = last.get('ops_per_second', 0)
            
            if 'concurrent_processing' in results and results['concurrent_processing']:
                last = results['concurrent_processing'][-1]
                metrics['throughput'] = last.get('throughput', 0)
        
        return metrics
    
    def _cleanup_old_records(self):
        """æ¸…ç†æ—§çš„å†å²è®°å½•"""
        history_files = sorted(self.history_dir.glob("history_*.json"))
        
        if len(history_files) > self.max_records:
            # åˆ é™¤æœ€æ—§çš„è®°å½•
            for old_file in history_files[:-self.max_records]:
                try:
                    old_file.unlink()
                    print(f"  ğŸ—‘ï¸  å·²åˆ é™¤æ—§è®°å½•: {old_file.name}")
                except Exception as e:
                    print(f"  âš ï¸  åˆ é™¤ {old_file.name} å¤±è´¥: {e}")
    
    def load_history(self, limit=10):
        """åŠ è½½å†å²è®°å½•"""
        history_files = sorted(self.history_dir.glob("history_*.json"), reverse=True)
        
        history_data = []
        
        for history_file in history_files[:limit]:
            try:
                with open(history_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    history_data.append(data)
            except Exception as e:
                print(f"  âš ï¸  åŠ è½½ {history_file.name} å¤±è´¥: {e}")
        
        return history_data
    
    def compare_with_baseline(self, baseline_date=None):
        """ä¸åŸºçº¿å¯¹æ¯”"""
        print("\nğŸ“Š æ€§èƒ½è¶‹åŠ¿åˆ†æ")
        print("=" * 70)
        
        history_data = self.load_history(limit=10)
        
        if len(history_data) < 2:
            print("å†å²æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œå¯¹æ¯”")
            return
        
        current = history_data[0]
        previous = history_data[1]
        
        print(f"\nå½“å‰æµ‹è¯•: {current['test_date']} {current['test_time']}")
        print(f"å¯¹æ¯”æµ‹è¯•: {previous['test_date']} {previous['test_time']}")
        print()
        
        # å¯¹æ¯”å„é¡¹æŒ‡æ ‡
        for report_type in current.get('reports', {}):
            if report_type not in previous.get('reports', {}):
                continue
            
            print(f"\nğŸ“‹ {report_type}")
            print("-" * 70)
            
            current_metrics = current['reports'][report_type]
            previous_metrics = previous['reports'][report_type]
            
            for metric_name in current_metrics:
                if metric_name not in previous_metrics:
                    continue
                
                current_value = current_metrics[metric_name]
                previous_value = previous_metrics[metric_name]
                
                if previous_value == 0:
                    continue
                
                change_percent = ((current_value - previous_value) / previous_value) * 100
                
                # åˆ¤æ–­å˜åŒ–æ–¹å‘
                if abs(change_percent) < 5:
                    icon = "â¡ï¸"
                    status = "æŒå¹³"
                elif change_percent > 0:
                    icon = "ğŸ“ˆ"
                    status = "æå‡"
                else:
                    icon = "ğŸ“‰"
                    status = "ä¸‹é™"
                
                print(f"  {icon} {metric_name}: {current_value:.2f} "
                      f"({status} {abs(change_percent):.2f}%)")
        
        # ç”Ÿæˆè¶‹åŠ¿æŠ¥å‘Š
        self._generate_trend_report(history_data)
    
    def _generate_trend_report(self, history_data):
        """ç”Ÿæˆè¶‹åŠ¿æŠ¥å‘Š"""
        trend_file = Path("test_results/performance_trend.txt")
        
        try:
            with open(trend_file, 'w', encoding='utf-8') as f:
                f.write("# æ€§èƒ½è¶‹åŠ¿æŠ¥å‘Š\n\n")
                f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"å†å²è®°å½•æ•°: {len(history_data)}\n\n")
                
                # å†™å…¥è¯¦ç»†æ•°æ®
                for i, record in enumerate(history_data):
                    f.write(f"\n## è®°å½• {i+1}: {record['test_date']} {record['test_time']}\n\n")
                    
                    for report_type, metrics in record.get('reports', {}).items():
                        f.write(f"### {report_type}\n")
                        for metric, value in metrics.items():
                            f.write(f"- {metric}: {value:.2f}\n")
                        f.write("\n")
            
            print(f"\nğŸ“„ è¶‹åŠ¿æŠ¥å‘Šå·²ä¿å­˜: {trend_file}")
        
        except Exception as e:
            print(f"âŒ ç”Ÿæˆè¶‹åŠ¿æŠ¥å‘Šå¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    tracker = HistoryTracker()
    
    # ä¿å­˜å½“å‰ç»“æœ
    tracker.save_current_results()
    
    # å¯¹æ¯”åˆ†æ
    tracker.compare_with_baseline()


if __name__ == "__main__":
    main()
