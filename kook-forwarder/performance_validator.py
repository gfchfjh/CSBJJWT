"""
æ€§èƒ½åŸºå‡†éªŒè¯å™¨
éªŒè¯æµ‹è¯•ç»“æœæ˜¯å¦è¾¾åˆ°æ€§èƒ½åŸºå‡†è¦æ±‚
"""
import json
import yaml
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any


class PerformanceValidator:
    """æ€§èƒ½éªŒè¯å™¨"""
    
    def __init__(self, config_path="stress_test_config.yaml"):
        self.config = self._load_config(config_path)
        self.benchmarks = self.config.get('benchmarks', {})
        self.requirements = self._extract_requirements()
        self.results = {
            'timestamp': datetime.now().isoformat(),
            'overall_pass': True,
            'checks': [],
            'grade': 'Unknown',
            'score': 0
        }
    
    def _load_config(self, config_path):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            print(f"âš ï¸  æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶: {e}")
            return {}
    
    def _extract_requirements(self):
        """æå–æ‰€æœ‰æ€§èƒ½è¦æ±‚"""
        requirements = {}
        
        # APIè¦æ±‚
        if 'api_tests' in self.config:
            requirements['api'] = self.config['api_tests'].get('requirements', {})
        
        # æ¶ˆæ¯å¤„ç†è¦æ±‚
        if 'message_processing' in self.config:
            requirements['message_processing'] = self.config['message_processing'].get('requirements', {})
        
        # Redisé˜Ÿåˆ—è¦æ±‚
        if 'redis_queue' in self.config:
            requirements['redis_queue'] = self.config['redis_queue'].get('requirements', {})
        
        # æ•°æ®åº“è¦æ±‚
        if 'database' in self.config:
            requirements['database'] = self.config['database'].get('requirements', {})
        
        return requirements
    
    def validate_all_reports(self):
        """éªŒè¯æ‰€æœ‰æµ‹è¯•æŠ¥å‘Š"""
        print("=" * 70)
        print("æ€§èƒ½åŸºå‡†éªŒè¯")
        print("=" * 70)
        
        test_results_dir = Path("test_results")
        if not test_results_dir.exists():
            print("âŒ test_results ç›®å½•ä¸å­˜åœ¨")
            return False
        
        report_files = list(test_results_dir.glob("*_report.json"))
        
        if not report_files:
            print("âŒ æœªæ‰¾åˆ°æµ‹è¯•æŠ¥å‘Š")
            return False
        
        print(f"\næ‰¾åˆ° {len(report_files)} ä¸ªæµ‹è¯•æŠ¥å‘Š\n")
        
        total_checks = 0
        passed_checks = 0
        
        for report_file in report_files:
            print(f"\nğŸ“„ éªŒè¯: {report_file.name}")
            print("-" * 70)
            
            data = self._load_report(report_file)
            if not data:
                continue
            
            # æ ¹æ®æŠ¥å‘Šç±»å‹è¿›è¡ŒéªŒè¯
            if 'comprehensive' in report_file.name:
                checks, passed = self._validate_comprehensive_report(data)
            elif 'demo' in report_file.name:
                checks, passed = self._validate_demo_report(data)
            elif 'module' in report_file.name:
                checks, passed = self._validate_module_report(data)
            else:
                continue
            
            total_checks += checks
            passed_checks += passed
            
            self.results['checks'].extend(self._get_last_checks())
        
        # è®¡ç®—æ€»ä½“ç»“æœ
        if total_checks > 0:
            pass_rate = (passed_checks / total_checks) * 100
            self.results['score'] = round(pass_rate, 2)
            self.results['overall_pass'] = pass_rate >= 80
            self.results['grade'] = self._calculate_grade(pass_rate)
        
        # æ‰“å°æ€»ç»“
        self._print_summary(total_checks, passed_checks)
        
        # ä¿å­˜éªŒè¯ç»“æœ
        self._save_results()
        
        return self.results['overall_pass']
    
    def _load_report(self, report_path):
        """åŠ è½½æµ‹è¯•æŠ¥å‘Š"""
        try:
            with open(report_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"  âŒ åŠ è½½æŠ¥å‘Šå¤±è´¥: {e}")
            return None
    
    def _validate_comprehensive_report(self, data):
        """éªŒè¯å…¨é¢æµ‹è¯•æŠ¥å‘Š"""
        checks = 0
        passed = 0
        
        if 'test_results' not in data:
            return checks, passed
        
        test_results = data['test_results']
        
        # éªŒè¯APIæ€§èƒ½
        if 'api_stress' in test_results and 'api' in self.requirements:
            c, p = self._validate_api_performance(test_results['api_stress'])
            checks += c
            passed += p
        
        # éªŒè¯æ ¼å¼è½¬æ¢æ€§èƒ½
        if 'formatter_stress' in test_results and 'message_processing' in self.requirements:
            c, p = self._validate_formatter_performance(test_results['formatter_stress'])
            checks += c
            passed += p
        
        # éªŒè¯Redisé˜Ÿåˆ—æ€§èƒ½
        if 'redis_queue_stress' in test_results and 'redis_queue' in self.requirements:
            c, p = self._validate_redis_performance(test_results['redis_queue_stress'])
            checks += c
            passed += p
        
        return checks, passed
    
    def _validate_api_performance(self, test_data):
        """éªŒè¯APIæ€§èƒ½"""
        checks = 0
        passed = 0
        
        if 'results' not in test_data:
            return checks, passed
        
        results = test_data['results']
        req = self.requirements.get('api', {})
        
        # æŸ¥æ‰¾å¹¶å‘100çš„ç»“æœ
        result_100 = None
        for r in results:
            if r.get('concurrent') == 100:
                result_100 = r
                break
        
        if result_100:
            # æ£€æŸ¥QPS
            if 'min_qps' in req:
                checks += 1
                actual_qps = result_100.get('qps', 0)
                min_qps = req['min_qps']
                
                if actual_qps >= min_qps:
                    print(f"  âœ… API QPS: {actual_qps:.2f} (>= {min_qps})")
                    passed += 1
                else:
                    print(f"  âŒ API QPS: {actual_qps:.2f} (< {min_qps})")
            
            # æ£€æŸ¥å¹³å‡å»¶è¿Ÿ
            if 'max_avg_latency_ms' in req:
                checks += 1
                actual_latency = result_100.get('avg_time_ms', 0)
                max_latency = req['max_avg_latency_ms']
                
                if actual_latency <= max_latency:
                    print(f"  âœ… å¹³å‡å»¶è¿Ÿ: {actual_latency:.2f}ms (<= {max_latency}ms)")
                    passed += 1
                else:
                    print(f"  âŒ å¹³å‡å»¶è¿Ÿ: {actual_latency:.2f}ms (> {max_latency}ms)")
            
            # æ£€æŸ¥æˆåŠŸç‡
            if 'min_success_rate' in req:
                checks += 1
                total = result_100.get('total_requests', 0)
                successful = result_100.get('successful', 0)
                
                if total > 0:
                    success_rate = (successful / total) * 100
                    min_rate = req['min_success_rate']
                    
                    if success_rate >= min_rate:
                        print(f"  âœ… æˆåŠŸç‡: {success_rate:.2f}% (>= {min_rate}%)")
                        passed += 1
                    else:
                        print(f"  âŒ æˆåŠŸç‡: {success_rate:.2f}% (< {min_rate}%)")
        
        return checks, passed
    
    def _validate_formatter_performance(self, test_data):
        """éªŒè¯æ ¼å¼è½¬æ¢æ€§èƒ½"""
        checks = 0
        passed = 0
        
        if 'results' not in test_data:
            return checks, passed
        
        results = test_data['results']
        req = self.requirements.get('message_processing', {})
        
        # å–æœ€åä¸€æ¬¡æµ‹è¯•ç»“æœï¼ˆæœ€å¤§è¿­ä»£æ¬¡æ•°ï¼‰
        last_result = results[-1] if results else None
        
        if last_result:
            # æ£€æŸ¥Discordè½¬æ¢æ€§èƒ½
            if 'discord_min_ops' in req and 'discord' in last_result:
                checks += 1
                actual_ops = last_result['discord'].get('ops_per_sec', 0)
                min_ops = req['discord_min_ops']
                
                if actual_ops >= min_ops:
                    print(f"  âœ… Discordè½¬æ¢: {actual_ops:.0f} ops/s (>= {min_ops})")
                    passed += 1
                else:
                    print(f"  âŒ Discordè½¬æ¢: {actual_ops:.0f} ops/s (< {min_ops})")
            
            # æ£€æŸ¥Telegramè½¬æ¢æ€§èƒ½
            if 'telegram_min_ops' in req and 'telegram' in last_result:
                checks += 1
                actual_ops = last_result['telegram'].get('ops_per_sec', 0)
                min_ops = req['telegram_min_ops']
                
                if actual_ops >= min_ops:
                    print(f"  âœ… Telegramè½¬æ¢: {actual_ops:.0f} ops/s (>= {min_ops})")
                    passed += 1
                else:
                    print(f"  âŒ Telegramè½¬æ¢: {actual_ops:.0f} ops/s (< {min_ops})")
        
        return checks, passed
    
    def _validate_redis_performance(self, test_data):
        """éªŒè¯Redisæ€§èƒ½"""
        checks = 0
        passed = 0
        
        if 'results' not in test_data:
            return checks, passed
        
        results = test_data['results']
        req = self.requirements.get('redis_queue', {})
        
        # æŸ¥æ‰¾æ‰¹é‡1000çš„ç»“æœ
        result_1000 = None
        for r in results:
            if r.get('batch_size') == 1000:
                result_1000 = r
                break
        
        if result_1000:
            # æ£€æŸ¥å…¥é˜Ÿæ€§èƒ½
            if 'min_enqueue_qps' in req:
                checks += 1
                actual_qps = result_1000.get('enqueue_qps', 0)
                min_qps = req['min_enqueue_qps']
                
                if actual_qps >= min_qps:
                    print(f"  âœ… å…¥é˜Ÿæ€§èƒ½: {actual_qps:.0f} msg/s (>= {min_qps})")
                    passed += 1
                else:
                    print(f"  âŒ å…¥é˜Ÿæ€§èƒ½: {actual_qps:.0f} msg/s (< {min_qps})")
            
            # æ£€æŸ¥å‡ºé˜Ÿæ€§èƒ½
            if 'min_dequeue_qps' in req:
                checks += 1
                actual_qps = result_1000.get('dequeue_qps', 0)
                min_qps = req['min_dequeue_qps']
                
                if actual_qps >= min_qps:
                    print(f"  âœ… å‡ºé˜Ÿæ€§èƒ½: {actual_qps:.0f} msg/s (>= {min_qps})")
                    passed += 1
                else:
                    print(f"  âŒ å‡ºé˜Ÿæ€§èƒ½: {actual_qps:.0f} msg/s (< {min_qps})")
        
        return checks, passed
    
    def _validate_demo_report(self, data):
        """éªŒè¯æ¼”ç¤ºæµ‹è¯•æŠ¥å‘Š"""
        # æ¼”ç¤ºæµ‹è¯•åªåšåŸºæœ¬éªŒè¯
        checks = 1
        passed = 0
        
        if 'results' in data and data['results']:
            print("  âœ… æ¼”ç¤ºæµ‹è¯•æ‰§è¡ŒæˆåŠŸ")
            passed = 1
        else:
            print("  âŒ æ¼”ç¤ºæµ‹è¯•æœªæ‰¾åˆ°ç»“æœ")
        
        return checks, passed
    
    def _validate_module_report(self, data):
        """éªŒè¯æ¨¡å—æµ‹è¯•æŠ¥å‘Š"""
        # æ¨¡å—æµ‹è¯•åšåŸºæœ¬éªŒè¯
        checks = len(data) if isinstance(data, dict) else 0
        passed = 0
        
        if isinstance(data, dict):
            for module, results in data.items():
                if results:
                    print(f"  âœ… {module} æ¨¡å—æµ‹è¯•æˆåŠŸ")
                    passed += 1
                else:
                    print(f"  âŒ {module} æ¨¡å—æµ‹è¯•å¤±è´¥")
        
        return checks, passed
    
    def _get_last_checks(self):
        """è·å–æœ€åçš„æ£€æŸ¥ç»“æœ"""
        # è¿™é‡Œå¯ä»¥å®ç°è¯¦ç»†çš„æ£€æŸ¥è®°å½•
        return []
    
    def _calculate_grade(self, pass_rate):
        """è®¡ç®—æ€§èƒ½ç­‰çº§"""
        grades = self.benchmarks.get('grades', {})
        
        if pass_rate >= grades.get('excellent', 120):
            return "ä¼˜ç§€ â­â­â­â­â­"
        elif pass_rate >= grades.get('good', 100):
            return "è‰¯å¥½ â­â­â­â­"
        elif pass_rate >= grades.get('acceptable', 80):
            return "åŠæ ¼ â­â­â­"
        else:
            return "å·® â­"
    
    def _print_summary(self, total_checks, passed_checks):
        """æ‰“å°éªŒè¯æ€»ç»“"""
        print("\n" + "=" * 70)
        print("éªŒè¯æ€»ç»“")
        print("=" * 70)
        
        if total_checks > 0:
            pass_rate = (passed_checks / total_checks) * 100
            
            print(f"\næ€»æ£€æŸ¥é¡¹: {total_checks}")
            print(f"é€šè¿‡æ£€æŸ¥: {passed_checks}")
            print(f"å¤±è´¥æ£€æŸ¥: {total_checks - passed_checks}")
            print(f"é€šè¿‡ç‡: {pass_rate:.2f}%")
            print(f"æ€§èƒ½ç­‰çº§: {self.results['grade']}")
            
            if self.results['overall_pass']:
                print("\nâœ… æ€§èƒ½éªŒè¯é€šè¿‡ï¼")
            else:
                print("\nâŒ æ€§èƒ½éªŒè¯æœªé€šè¿‡ï¼")
        else:
            print("\nâš ï¸  æœªæ‰¾åˆ°å¯éªŒè¯çš„æ•°æ®")
    
    def _save_results(self):
        """ä¿å­˜éªŒè¯ç»“æœ"""
        output_path = Path("test_results/performance_validation.json")
        
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, indent=2, ensure_ascii=False)
            print(f"\nğŸ’¾ éªŒè¯ç»“æœå·²ä¿å­˜: {output_path}")
        except Exception as e:
            print(f"\nâŒ ä¿å­˜éªŒè¯ç»“æœå¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    validator = PerformanceValidator()
    result = validator.validate_all_reports()
    
    # è¿”å›é€€å‡ºç 
    import sys
    sys.exit(0 if result else 1)


if __name__ == "__main__":
    main()
