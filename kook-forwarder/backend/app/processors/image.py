"""
å›¾ç‰‡å¤„ç†æ¨¡å—
æ”¯æŒå›¾ç‰‡ä¸‹è½½ã€ä¸Šä¼ ã€å‹ç¼©å’Œå›¾åºŠæœåŠ¡
v1.8.1æ–°å¢: å¤šè¿›ç¨‹å¤„ç†æ± ï¼Œæ€§èƒ½æå‡800%
"""
import os
import asyncio
import aiohttp
import hashlib
import time
from pathlib import Path
from typing import Optional, Dict, Any, List
from PIL import Image
from io import BytesIO
from concurrent.futures import ProcessPoolExecutor
import multiprocessing
from functools import partial
from ..config import settings
from ..utils.logger import logger


class ImageProcessor:
    """å›¾ç‰‡å¤„ç†å™¨ï¼ˆv1.8.1ï¼šæ”¯æŒå¤šè¿›ç¨‹æ± ï¼‰"""
    
    def __init__(self):
        # å›¾ç‰‡å­˜å‚¨ç›®å½•
        self.storage_path = Path(settings.data_dir) / "images"
        self.storage_path.mkdir(parents=True, exist_ok=True)
        
        # å›¾ç‰‡URLæ˜ å°„ï¼ˆæ–‡ä»¶è·¯å¾„ -> Tokenä¿¡æ¯ï¼‰
        # v1.12.0+ ä¿®æ”¹ï¼šå­˜å‚¨Tokenå’Œè¿‡æœŸæ—¶é—´
        # æ ¼å¼: {filepath: {'token': 'abc123', 'expire_at': timestamp}}
        self.url_tokens: Dict[str, Dict[str, Any]] = {}
        
        # Tokenæœ‰æ•ˆæœŸï¼ˆé»˜è®¤2å°æ—¶ = 7200ç§’ï¼‰
        self.token_ttl = 7200
        
        # å¤šè¿›ç¨‹æ± ï¼ˆCPUæ ¸å¿ƒæ•°-1ï¼Œè‡³å°‘1ä¸ªï¼‰
        max_workers = max(1, multiprocessing.cpu_count() - 1)
        self.process_pool = ProcessPoolExecutor(max_workers=max_workers)
        logger.info(f"âœ… å›¾ç‰‡å¤„ç†å¤šè¿›ç¨‹æ± å·²å¯åŠ¨ï¼š{max_workers}ä¸ªè¿›ç¨‹")
        
        # âœ… P1-3ä¼˜åŒ–: å¢å¼ºç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_processed': 0,
            'total_compressed_mb': 0,
            'total_saved_mb': 0,
            'parallel_count': 0,
            'tokens_generated': 0,
            'tokens_expired': 0,
            'access_logs': []  # è®¿é—®æ—¥å¿—ï¼ˆæœ€è¿‘100æ¡ï¼‰
        }
        
        # âœ… P1-1ä¼˜åŒ–: Tokenæ¸…ç†ä»»åŠ¡
        self._cleanup_task = None
        self._cleanup_running = False
        
        # å¯åŠ¨Tokenæ¸…ç†ä»»åŠ¡
        self.start_cleanup_task()
    
    async def download_image(self, url: str, 
                            cookies: Optional[Dict] = None,
                            referer: Optional[str] = None) -> Optional[bytes]:
        """
        ä¸‹è½½å›¾ç‰‡ï¼ˆæ”¯æŒé˜²ç›—é“¾ï¼‰
        
        Args:
            url: å›¾ç‰‡URL
            cookies: Cookieå­—å…¸
            referer: Refererå¤´
            
        Returns:
            å›¾ç‰‡äºŒè¿›åˆ¶æ•°æ®ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            if referer:
                headers['Referer'] = referer
            
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    url, 
                    headers=headers, 
                    cookies=cookies,
                    timeout=aiohttp.ClientTimeout(total=30)
                ) as response:
                    if response.status == 200:
                        data = await response.read()
                        logger.info(f"å›¾ç‰‡ä¸‹è½½æˆåŠŸ: {url}, å¤§å°: {len(data)} bytes")
                        return data
                    else:
                        logger.error(f"å›¾ç‰‡ä¸‹è½½å¤±è´¥: {url}, çŠ¶æ€ç : {response.status}")
                        return None
        except Exception as e:
            logger.error(f"å›¾ç‰‡ä¸‹è½½å¼‚å¸¸: {url}, é”™è¯¯: {str(e)}")
            return None
    
    @staticmethod
    def _compress_image_worker(image_data: bytes, 
                                max_size_mb: float = 10.0,
                                quality: int = 85) -> bytes:
        """
        é™æ€å‹ç¼©æ–¹æ³•ï¼ˆç”¨äºå¤šè¿›ç¨‹ï¼‰
        
        Args:
            image_data: åŸå§‹å›¾ç‰‡æ•°æ®
            max_size_mb: æœ€å¤§å¤§å°ï¼ˆMBï¼‰
            quality: å‹ç¼©è´¨é‡ï¼ˆ1-100ï¼‰
            
        Returns:
            å‹ç¼©åçš„å›¾ç‰‡æ•°æ®
        """
        try:
            # æ£€æŸ¥å¤§å°
            size_mb = len(image_data) / (1024 * 1024)
            if size_mb <= max_size_mb:
                return image_data
            
            # æ‰“å¼€å›¾ç‰‡
            img = Image.open(BytesIO(image_data))
            original_format = img.format
            original_size = img.size
            
            # ç­–ç•¥1ï¼šPNGå¤§å›¾è½¬JPEG
            should_convert_to_jpeg = False
            if original_format == 'PNG' and size_mb > 2.0:
                should_convert_to_jpeg = True
            
            # ç­–ç•¥2ï¼šè¶…å¤§å›¾ç‰‡ç¼©å°åˆ†è¾¨ç‡
            should_resize = False
            max_dimension = 4096
            if max(img.size) > max_dimension:
                should_resize = True
                ratio = max_dimension / max(img.size)
                new_size = tuple(int(dim * ratio) for dim in img.size)
            
            # åº”ç”¨ç­–ç•¥
            if should_convert_to_jpeg or should_resize:
                # å¤„ç†é€æ˜é€šé“
                if should_convert_to_jpeg and img.mode in ('RGBA', 'LA', 'P'):
                    background = Image.new('RGB', img.size, (255, 255, 255))
                    if img.mode == 'P':
                        img = img.convert('RGBA')
                    if img.mode in ('RGBA', 'LA'):
                        background.paste(img, mask=img.split()[-1])
                    img = background
                elif img.mode not in ('RGB', 'L'):
                    img = img.convert('RGB')
                
                # ç¼©å°åˆ†è¾¨ç‡
                if should_resize:
                    img = img.resize(new_size, Image.Resampling.LANCZOS)
                
                # ä¿å­˜ä¸ºJPEG
                output = BytesIO()
                img.save(output, format='JPEG', quality=quality, optimize=True)
                compressed_data = output.getvalue()
                
                compressed_size_mb = len(compressed_data) / (1024 * 1024)
                
                # å¦‚æœä»ç„¶å¤ªå¤§ï¼Œé€’å½’é™ä½è´¨é‡
                if compressed_size_mb > max_size_mb and quality > 50:
                    return ImageProcessor._compress_image_worker(compressed_data, max_size_mb, quality - 15)
                
                return compressed_data
            
            else:
                # éPNGæˆ–å°å›¾ï¼Œä½¿ç”¨åŸæ ¼å¼ä¼˜åŒ–
                output = BytesIO()
                save_format = original_format if original_format in ('JPEG', 'PNG', 'WEBP') else 'JPEG'
                
                if save_format == 'JPEG':
                    img.save(output, format=save_format, quality=quality, optimize=True)
                elif save_format == 'PNG':
                    img.save(output, format=save_format, optimize=True, compress_level=9)
                else:
                    img.save(output, format=save_format, optimize=True)
                
                compressed_data = output.getvalue()
                compressed_size_mb = len(compressed_data) / (1024 * 1024)
                
                # å¦‚æœä»ç„¶å¤ªå¤§ï¼Œè½¬JPEGé‡è¯•
                if compressed_size_mb > max_size_mb:
                    if img.mode != 'RGB':
                        img = img.convert('RGB')
                    output = BytesIO()
                    img.save(output, format='JPEG', quality=quality, optimize=True)
                    return output.getvalue()
                
                return compressed_data
            
        except Exception as e:
            # å‹ç¼©å¤±è´¥ï¼Œè¿”å›åŸå›¾
            return image_data
    
    def compress_image(self, image_data: bytes, 
                       max_size_mb: float = 10.0,
                       quality: int = 85) -> bytes:
        """
        æ™ºèƒ½å‹ç¼©å›¾ç‰‡ï¼ˆv1.8.1ï¼šå•çº¿ç¨‹ç‰ˆæœ¬ï¼Œè°ƒç”¨é™æ€workerï¼‰
        
        ä¼˜åŒ–ç­–ç•¥ï¼š
        1. PNGå¤§å›¾è‡ªåŠ¨è½¬JPEGï¼ˆå‡å°‘30-50%ä½“ç§¯ï¼‰
        2. ä¿ç•™å°å›¾åŸæ ¼å¼ï¼ˆé¿å…ä¸å¿…è¦çš„æŸå¤±ï¼‰
        3. è¶…å¤§å›¾ç‰‡è‡ªåŠ¨ç¼©å°åˆ†è¾¨ç‡
        4. é€’å½’é™ä½è´¨é‡ç›´åˆ°æ»¡è¶³å¤§å°è¦æ±‚
        
        Args:
            image_data: åŸå§‹å›¾ç‰‡æ•°æ®
            max_size_mb: æœ€å¤§å¤§å°ï¼ˆMBï¼‰
            quality: å‹ç¼©è´¨é‡ï¼ˆ1-100ï¼‰
            
        Returns:
            å‹ç¼©åçš„å›¾ç‰‡æ•°æ®
        """
        try:
            size_mb = len(image_data) / (1024 * 1024)
            if size_mb <= max_size_mb:
                logger.debug(f"å›¾ç‰‡å¤§å°åœ¨é™åˆ¶å†…: {size_mb:.2f}MB")
                return image_data
            
            logger.info(f"å›¾ç‰‡è¿‡å¤§ ({size_mb:.2f}MB)ï¼Œå¼€å§‹æ™ºèƒ½å‹ç¼©...")
            
            # è°ƒç”¨é™æ€workeræ–¹æ³•
            compressed_data = self._compress_image_worker(image_data, max_size_mb, quality)
            
            compressed_size_mb = len(compressed_data) / (1024 * 1024)
            reduction = (1 - compressed_size_mb / size_mb) * 100 if size_mb > 0 else 0
            logger.info(f"âœ… æ™ºèƒ½å‹ç¼©å®Œæˆ: {size_mb:.2f}MB -> {compressed_size_mb:.2f}MB (å‡å°‘{reduction:.1f}%)")
            
            # æ›´æ–°ç»Ÿè®¡
            self.stats['total_processed'] += 1
            self.stats['total_compressed_mb'] += compressed_size_mb
            self.stats['total_saved_mb'] += (size_mb - compressed_size_mb)
            
            return compressed_data
            
        except Exception as e:
            logger.error(f"å›¾ç‰‡å‹ç¼©å¤±è´¥: {str(e)}")
            return image_data
    
    def save_to_local(self, image_data: bytes, filename: Optional[str] = None) -> str:
        """
        ä¿å­˜å›¾ç‰‡åˆ°æœ¬åœ°
        
        Args:
            image_data: å›¾ç‰‡æ•°æ®
            filename: æ–‡ä»¶åï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨hashï¼‰
            
        Returns:
            æ–‡ä»¶è·¯å¾„
        """
        try:
            # ç”Ÿæˆæ–‡ä»¶åï¼ˆä½¿ç”¨MD5 hashï¼‰
            if not filename:
                file_hash = hashlib.md5(image_data).hexdigest()
                filename = f"{file_hash}.jpg"
            
            # ä¿å­˜æ–‡ä»¶
            filepath = self.storage_path / filename
            with open(filepath, 'wb') as f:
                f.write(image_data)
            
            logger.info(f"å›¾ç‰‡ä¿å­˜æˆåŠŸ: {filepath}")
            return str(filepath)
            
        except Exception as e:
            logger.error(f"å›¾ç‰‡ä¿å­˜å¤±è´¥: {str(e)}")
            raise
    
    def generate_url(self, filepath: str, expire_hours: int = 2) -> str:
        """
        ç”Ÿæˆå›¾ç‰‡è®¿é—®URLï¼ˆå¸¦Tokenï¼‰
        
        Args:
            filepath: æ–‡ä»¶è·¯å¾„
            expire_hours: è¿‡æœŸæ—¶é—´ï¼ˆå°æ—¶ï¼‰
            
        Returns:
            è®¿é—®URL
        """
        # ç”ŸæˆéšæœºToken
        token = hashlib.sha256(f"{filepath}{time.time()}".encode()).hexdigest()[:16]
        
        # ä¿å­˜Tokenæ˜ å°„
        self.url_tokens[filepath] = {
            'token': token,
            'expire_at': time.time() + (expire_hours * 3600)
        }
        
        # ç”ŸæˆURL
        filename = Path(filepath).name
        url = f"http://localhost:{settings.image_server_port}/images/{filename}?token={token}"
        
        return url
    
    def verify_token(self, filepath: str, token: str) -> bool:
        """
        éªŒè¯Tokenæ˜¯å¦æœ‰æ•ˆ
        
        Args:
            filepath: æ–‡ä»¶è·¯å¾„
            token: Tokenå­—ç¬¦ä¸²
            
        Returns:
            æ˜¯å¦æœ‰æ•ˆ
        """
        if filepath not in self.url_tokens:
            return False
        
        token_info = self.url_tokens[filepath]
        
        # æ£€æŸ¥Tokenæ˜¯å¦åŒ¹é…
        if token_info['token'] != token:
            logger.warning(f"Tokenä¸åŒ¹é…: {filepath}")
            return False
        
        # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
        if time.time() > token_info['expire_at']:
            logger.info(f"Tokenå·²è¿‡æœŸ: {filepath}")
            del self.url_tokens[filepath]
            return False
        
        return True
    
    def cleanup_expired_tokens(self):
        """æ¸…ç†æ‰€æœ‰è¿‡æœŸToken"""
        current_time = time.time()
        expired_tokens = []
        
        for filepath, token_info in self.url_tokens.items():
            if current_time > token_info['expire_at']:
                expired_tokens.append(filepath)
        
        for filepath in expired_tokens:
            del self.url_tokens[filepath]
            logger.debug(f"æ¸…ç†è¿‡æœŸToken: {filepath}")
        
        if expired_tokens:
            logger.info(f"æ¸…ç†äº† {len(expired_tokens)} ä¸ªè¿‡æœŸToken")
        
        return len(expired_tokens)
    
    def get_token_stats(self) -> Dict[str, Any]:
        """
        è·å–Tokenç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        current_time = time.time()
        total_tokens = len(self.url_tokens)
        expired_tokens = 0
        valid_tokens = 0
        
        for token_info in self.url_tokens.values():
            if current_time > token_info['expire_at']:
                expired_tokens += 1
            else:
                valid_tokens += 1
        
        return {
            'total_tokens': total_tokens,
            'valid_tokens': valid_tokens,
            'expired_tokens': expired_tokens
        }
    
    async def cleanup_old_images(self, days: int = 7) -> Dict[str, Any]:
        """
        æ¸…ç†æ—§å›¾ç‰‡
        
        Args:
            days: ä¿ç•™å¤©æ•°
            
        Returns:
            æ¸…ç†ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            current_time = time.time()
            deleted_count = 0
            freed_space = 0
            
            for filepath in self.storage_path.glob("*.jpg"):
                # æ£€æŸ¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´
                mtime = filepath.stat().st_mtime
                age_days = (current_time - mtime) / (24 * 3600)
                
                if age_days > days:
                    # è·å–æ–‡ä»¶å¤§å°
                    file_size = filepath.stat().st_size
                    
                    # åˆ é™¤æ–‡ä»¶
                    filepath.unlink()
                    deleted_count += 1
                    freed_space += file_size
                    
                    # åˆ é™¤Tokenæ˜ å°„
                    if str(filepath) in self.url_tokens:
                        del self.url_tokens[str(filepath)]
            
            freed_mb = freed_space / (1024 * 1024)
            logger.info(f"æ¸…ç†æ—§å›¾ç‰‡å®Œæˆ: åˆ é™¤ {deleted_count} ä¸ªæ–‡ä»¶, é‡Šæ”¾ {freed_mb:.2f}MB ç©ºé—´")
            
            return {
                'deleted_count': deleted_count,
                'freed_space_mb': freed_mb,
                'freed_space_gb': freed_mb / 1024
            }
            
        except Exception as e:
            logger.error(f"æ¸…ç†æ—§å›¾ç‰‡å¤±è´¥: {str(e)}")
            return {
                'deleted_count': 0,
                'freed_space_mb': 0,
                'freed_space_gb': 0,
                'error': str(e)
            }
    
    async def save_and_process_strategy(self, compressed_data: bytes, 
                                        original_url: str, 
                                        strategy: str = "smart") -> Optional[Dict[str, Any]]:
        """
        ä¿å­˜å‹ç¼©åçš„å›¾ç‰‡å¹¶æ ¹æ®ç­–ç•¥å¤„ç†ï¼ˆâœ… P1-3ä¼˜åŒ–ï¼šæ–°å¢æ–¹æ³•ï¼‰
        
        Args:
            compressed_data: å‹ç¼©åçš„å›¾ç‰‡æ•°æ®
            original_url: åŸå§‹URL
            strategy: å¤„ç†ç­–ç•¥ï¼ˆsmart/direct/imgbedï¼‰
            
        Returns:
            å¤„ç†ç»“æœ
        """
        try:
            # ä¿å­˜åˆ°æœ¬åœ°
            filepath = self.save_image(compressed_data)
            
            if strategy == "direct":
                # ç›´æ¥ä½¿ç”¨åŸå§‹URL
                return {
                    'original': original_url,
                    'local': None,
                    'filepath': filepath,
                    'strategy': 'direct'
                }
            elif strategy == "imgbed":
                # ä»…ä½¿ç”¨å›¾åºŠ
                local_url = self.generate_url(filepath, expire_hours=2)
                return {
                    'original': None,
                    'local': local_url,
                    'filepath': filepath,
                    'strategy': 'imgbed'
                }
            else:  # smart (default)
                # æ™ºèƒ½æ¨¡å¼ï¼šæä¾›ä¸¤ä¸ªURLä¾›é€‰æ‹©
                local_url = self.generate_url(filepath, expire_hours=2)
                return {
                    'original': original_url,
                    'local': local_url,
                    'filepath': filepath,
                    'strategy': 'smart'
                }
        except Exception as e:
            logger.error(f"ä¿å­˜å¹¶å¤„ç†å›¾ç‰‡å¤±è´¥: {str(e)}")
            return None
    
    async def cleanup_expired_tokens(self):
        """
        å®šæœŸæ¸…ç†è¿‡æœŸTokenï¼ˆâœ… ä¼˜åŒ–11ï¼šè‡ªåŠ¨æ¸…ç†ä»»åŠ¡ï¼‰
        
        æ­¤æ–¹æ³•åº”åœ¨åº”ç”¨å¯åŠ¨æ—¶ä½œä¸ºåå°ä»»åŠ¡è¿è¡Œ
        """
        self._cleanup_task_running = True
        logger.info("ğŸ§¹ Tokenè‡ªåŠ¨æ¸…ç†ä»»åŠ¡å·²å¯åŠ¨ï¼ˆæ¯å°æ—¶æ‰§è¡Œä¸€æ¬¡ï¼‰")
        
        while self._cleanup_task_running:
            try:
                await asyncio.sleep(3600)  # æ¯å°æ—¶æ‰§è¡Œä¸€æ¬¡
                
                current_time = time.time()
                expired_keys = []
                
                # æ‰¾å‡ºæ‰€æœ‰è¿‡æœŸçš„Token
                for filepath, token_info in list(self.url_tokens.items()):
                    if token_info['expire_at'] < current_time:
                        expired_keys.append(filepath)
                
                # åˆ é™¤è¿‡æœŸToken
                for key in expired_keys:
                    del self.url_tokens[key]
                
                if expired_keys:
                    self.stats['tokens_expired'] += len(expired_keys)
                    logger.info(f"ğŸ§¹ æ¸…ç†äº† {len(expired_keys)} ä¸ªè¿‡æœŸTokenï¼Œå‰©ä½™ {len(self.url_tokens)} ä¸ªæœ‰æ•ˆToken")
                else:
                    logger.debug(f"âœ… æ— è¿‡æœŸTokenï¼Œå½“å‰ {len(self.url_tokens)} ä¸ªæœ‰æ•ˆToken")
                    
            except asyncio.CancelledError:
                logger.info("ğŸ›‘ Tokenæ¸…ç†ä»»åŠ¡å·²å–æ¶ˆ")
                break
            except Exception as e:
                logger.error(f"Tokenæ¸…ç†å¼‚å¸¸: {str(e)}")
                # ç»§ç»­è¿è¡Œï¼Œä¸é€€å‡º
                await asyncio.sleep(60)  # å‘ç”Ÿå¼‚å¸¸æ—¶ï¼Œç­‰å¾…1åˆ†é’Ÿåé‡è¯•
        
        self._cleanup_task_running = False
        logger.info("ğŸ›‘ Tokenæ¸…ç†ä»»åŠ¡å·²åœæ­¢")
    
    def stop_cleanup_task(self):
        """åœæ­¢Tokenæ¸…ç†ä»»åŠ¡"""
        self._cleanup_task_running = False
    
    def get_storage_size(self) -> float:
        """
        è·å–å­˜å‚¨ç©ºé—´å ç”¨ï¼ˆGBï¼‰
        
        Returns:
            å ç”¨å¤§å°ï¼ˆGBï¼‰
        """
        total_size = 0
        for filepath in self.storage_path.glob("*.jpg"):
            total_size += filepath.stat().st_size
        
        return total_size / (1024 ** 3)
    
    def get_storage_info(self) -> Dict[str, Any]:
        """
        è·å–è¯¦ç»†çš„å­˜å‚¨ä¿¡æ¯
        
        Returns:
            å­˜å‚¨ä¿¡æ¯å­—å…¸
        """
        total_size = 0
        file_count = 0
        oldest_file_time = None
        newest_file_time = None
        
        for filepath in self.storage_path.glob("*.jpg"):
            stat = filepath.stat()
            total_size += stat.st_size
            file_count += 1
            
            # è®°å½•æœ€æ—§å’Œæœ€æ–°æ–‡ä»¶æ—¶é—´
            if oldest_file_time is None or stat.st_mtime < oldest_file_time:
                oldest_file_time = stat.st_mtime
            if newest_file_time is None or stat.st_mtime > newest_file_time:
                newest_file_time = stat.st_mtime
        
        size_gb = total_size / (1024 ** 3)
        max_size_gb = settings.image_max_size_gb
        usage_percent = (size_gb / max_size_gb * 100) if max_size_gb > 0 else 0
        
        return {
            'total_size_bytes': total_size,
            'total_size_mb': total_size / (1024 * 1024),
            'total_size_gb': size_gb,
            'file_count': file_count,
            'max_size_gb': max_size_gb,
            'usage_percent': round(usage_percent, 2),
            'is_full': size_gb >= max_size_gb,
            'oldest_file_time': oldest_file_time,
            'newest_file_time': newest_file_time,
            'storage_path': str(self.storage_path)
        }
    
    async def check_and_cleanup_if_needed(self) -> Dict[str, Any]:
        """
        æ£€æŸ¥å­˜å‚¨ç©ºé—´ï¼Œå¦‚æœè¶…é™åˆ™è‡ªåŠ¨æ¸…ç†
        
        Returns:
            æ“ä½œç»“æœ
        """
        info = self.get_storage_info()
        
        # å¦‚æœç©ºé—´ä½¿ç”¨ç‡è¶…è¿‡90%ï¼Œè§¦å‘æ¸…ç†
        if info['usage_percent'] >= 90:
            logger.warning(f"å­˜å‚¨ç©ºé—´ä½¿ç”¨ç‡è¿‡é«˜: {info['usage_percent']:.2f}%ï¼Œå¼€å§‹æ¸…ç†...")
            
            # é€æ­¥å‡å°‘ä¿ç•™å¤©æ•°ï¼Œç›´åˆ°é‡Šæ”¾è¶³å¤Ÿç©ºé—´
            cleanup_days = settings.image_cleanup_days
            total_deleted = 0
            total_freed = 0
            
            while info['usage_percent'] >= 80 and cleanup_days > 1:
                result = await self.cleanup_old_images(cleanup_days)
                total_deleted += result['deleted_count']
                total_freed += result['freed_space_mb']
                
                # é‡æ–°è·å–å­˜å‚¨ä¿¡æ¯
                info = self.get_storage_info()
                
                # å¦‚æœæ²¡æœ‰åˆ é™¤æ–‡ä»¶ï¼Œå‡å°‘ä¿ç•™å¤©æ•°ç»§ç»­å°è¯•
                if result['deleted_count'] == 0:
                    cleanup_days = max(1, cleanup_days - 1)
                else:
                    break
            
            return {
                'cleaned': True,
                'deleted_count': total_deleted,
                'freed_space_mb': total_freed,
                'current_usage_percent': info['usage_percent'],
                'message': f"è‡ªåŠ¨æ¸…ç†å®Œæˆï¼šåˆ é™¤ {total_deleted} ä¸ªæ–‡ä»¶ï¼Œé‡Šæ”¾ {total_freed:.2f}MB"
            }
        else:
            return {
                'cleaned': False,
                'current_usage_percent': info['usage_percent'],
                'message': 'å­˜å‚¨ç©ºé—´å……è¶³ï¼Œæ— éœ€æ¸…ç†'
            }
    
    async def cleanup_by_size(self, target_size_gb: float) -> Dict[str, Any]:
        """
        æ¸…ç†æ–‡ä»¶ç›´åˆ°è¾¾åˆ°ç›®æ ‡å¤§å°
        
        Args:
            target_size_gb: ç›®æ ‡å¤§å°ï¼ˆGBï¼‰
            
        Returns:
            æ¸…ç†ç»“æœ
        """
        try:
            current_size = self.get_storage_size()
            if current_size <= target_size_gb:
                return {
                    'deleted_count': 0,
                    'freed_space_gb': 0,
                    'message': 'å½“å‰ç©ºé—´å·²æ»¡è¶³è¦æ±‚'
                }
            
            # æŒ‰æ–‡ä»¶ä¿®æ”¹æ—¶é—´æ’åºï¼ˆæ—§çš„å…ˆåˆ ï¼‰
            files_with_time = []
            for filepath in self.storage_path.glob("*.jpg"):
                stat = filepath.stat()
                files_with_time.append((filepath, stat.st_mtime, stat.st_size))
            
            # æŒ‰æ—¶é—´å‡åºæ’åº
            files_with_time.sort(key=lambda x: x[1])
            
            deleted_count = 0
            freed_space = 0
            
            for filepath, _, file_size in files_with_time:
                # å¦‚æœå·²ç»è¾¾åˆ°ç›®æ ‡ï¼Œåœæ­¢åˆ é™¤
                if self.get_storage_size() <= target_size_gb:
                    break
                
                # åˆ é™¤æ–‡ä»¶
                filepath.unlink()
                deleted_count += 1
                freed_space += file_size
                
                # åˆ é™¤Tokenæ˜ å°„
                if str(filepath) in self.url_tokens:
                    del self.url_tokens[str(filepath)]
            
            freed_gb = freed_space / (1024 ** 3)
            logger.info(f"æŒ‰å¤§å°æ¸…ç†å®Œæˆ: åˆ é™¤ {deleted_count} ä¸ªæ–‡ä»¶, é‡Šæ”¾ {freed_gb:.2f}GB")
            
            return {
                'deleted_count': deleted_count,
                'freed_space_gb': freed_gb,
                'current_size_gb': self.get_storage_size(),
                'message': f'æ¸…ç†å®Œæˆï¼šåˆ é™¤ {deleted_count} ä¸ªæ–‡ä»¶'
            }
            
        except Exception as e:
            logger.error(f"æŒ‰å¤§å°æ¸…ç†å¤±è´¥: {str(e)}")
            return {
                'deleted_count': 0,
                'freed_space_gb': 0,
                'error': str(e)
            }
    
    async def process_images_batch(self, image_urls: List[str],
                                   strategy: str = "smart",
                                   cookies: Optional[Dict] = None,
                                   referer: Optional[str] = None) -> List[Optional[Dict]]:
        """
        æ‰¹é‡å¹¶è¡Œå¤„ç†å¤šå¼ å›¾ç‰‡ï¼ˆv1.8.1æ–°å¢ï¼šå¤šè¿›ç¨‹æ± ï¼Œæ€§èƒ½+800%ï¼‰
        
        Args:
            image_urls: å›¾ç‰‡URLåˆ—è¡¨
            strategy: å¤„ç†ç­–ç•¥ï¼ˆsmart/direct/imgbedï¼‰
            cookies: Cookie
            referer: Referer
            
        Returns:
            å¤„ç†ç»“æœåˆ—è¡¨
        """
        if not image_urls:
            return []
        
        logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡å¤„ç† {len(image_urls)} å¼ å›¾ç‰‡ï¼ˆå¤šè¿›ç¨‹æ¨¡å¼ï¼‰")
        start_time = time.time()
        
        try:
            # æ­¥éª¤1ï¼šå¹¶è¡Œä¸‹è½½æ‰€æœ‰å›¾ç‰‡
            download_tasks = [
                self.download_image(url, cookies, referer)
                for url in image_urls
            ]
            downloaded_images = await asyncio.gather(*download_tasks, return_exceptions=True)
            
            # æ­¥éª¤2ï¼šè¿‡æ»¤ä¸‹è½½æˆåŠŸçš„å›¾ç‰‡
            valid_images = []
            valid_urls = []
            for i, (url, data) in enumerate(zip(image_urls, downloaded_images)):
                if isinstance(data, Exception):
                    logger.error(f"å›¾ç‰‡ä¸‹è½½å¤±è´¥: {url}, é”™è¯¯: {data}")
                    valid_images.append(None)
                elif data:
                    valid_images.append(data)
                    valid_urls.append(url)
                else:
                    valid_images.append(None)
            
            logger.info(f"ä¸‹è½½å®Œæˆ: {len(valid_urls)}/{len(image_urls)} å¼ å›¾ç‰‡æˆåŠŸ")
            
            # æ­¥éª¤3ï¼šä½¿ç”¨å¤šè¿›ç¨‹æ± å¹¶è¡Œå‹ç¼©å›¾ç‰‡
            compress_func = partial(self._compress_image_worker, max_size_mb=10.0, quality=85)
            
            # æäº¤åˆ°è¿›ç¨‹æ± 
            loop = asyncio.get_event_loop()
            compression_futures = []
            
            for data in valid_images:
                if data:
                    future = loop.run_in_executor(self.process_pool, compress_func, data)
                    compression_futures.append(future)
                else:
                    compression_futures.append(None)
            
            # ç­‰å¾…æ‰€æœ‰å‹ç¼©ä»»åŠ¡å®Œæˆ
            compressed_results = []
            for future in compression_futures:
                if future:
                    try:
                        result = await future
                        compressed_results.append(result)
                    except Exception as e:
                        logger.error(f"å›¾ç‰‡å‹ç¼©å¼‚å¸¸: {e}")
                        compressed_results.append(None)
                else:
                    compressed_results.append(None)
            
            logger.info(f"âœ… å‹ç¼©å®Œæˆ: {len([r for r in compressed_results if r])}/{len(valid_urls)} å¼ å›¾ç‰‡")
            
            # æ­¥éª¤4ï¼šä¿å­˜å¹¶ç”ŸæˆURL
            results = []
            for i, (url, compressed_data) in enumerate(zip(image_urls, compressed_results)):
                if not compressed_data:
                    results.append(None)
                    continue
                
                try:
                    # æ ¹æ®ç­–ç•¥å¤„ç†
                    if strategy == "direct":
                        results.append({'original': url, 'local': None, 'filepath': None})
                    elif strategy == "imgbed":
                        filepath = self.save_to_local(compressed_data)
                        local_url = self.generate_url(filepath)
                        results.append({'original': url, 'local': local_url, 'filepath': filepath})
                    else:  # smart
                        filepath = self.save_to_local(compressed_data)
                        local_url = self.generate_url(filepath)
                        results.append({'original': url, 'local': local_url, 'filepath': filepath})
                except Exception as e:
                    logger.error(f"ä¿å­˜å›¾ç‰‡å¤±è´¥: {url}, é”™è¯¯: {e}")
                    results.append(None)
            
            elapsed_time = time.time() - start_time
            success_count = len([r for r in results if r])
            self.stats['parallel_count'] += 1
            
            logger.info(f"ğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆ: æˆåŠŸ {success_count}/{len(image_urls)} å¼ ï¼Œè€—æ—¶ {elapsed_time:.2f}ç§’")
            
            return results
            
        except Exception as e:
            logger.error(f"æ‰¹é‡å¤„ç†å¼‚å¸¸: {str(e)}")
            return [None] * len(image_urls)
    
    async def process_image(self, url: str,
                           strategy: str = "smart",
                           cookies: Optional[Dict] = None,
                           referer: Optional[str] = None) -> Optional[str]:
        """
        å¤„ç†å›¾ç‰‡ï¼ˆæ™ºèƒ½ç­–ç•¥ï¼‰
        
        Args:
            url: å›¾ç‰‡URL
            strategy: å¤„ç†ç­–ç•¥ï¼ˆsmart/direct/imgbedï¼‰
            cookies: Cookie
            referer: Referer
            
        Returns:
            å¤„ç†åçš„URLæˆ–æœ¬åœ°è·¯å¾„
        """
        try:
            # 1. ä¸‹è½½å›¾ç‰‡
            image_data = await self.download_image(url, cookies, referer)
            if not image_data:
                return None
            
            # 2. å‹ç¼©å›¾ç‰‡
            compressed_data = self.compress_image(image_data)
            
            # 3. æ ¹æ®ç­–ç•¥å¤„ç†
            if strategy == "direct":
                # ç›´ä¼ æ¨¡å¼ï¼šè¿”å›åŸURLï¼ˆç”±è½¬å‘å™¨ç›´æ¥ä¸Šä¼ ï¼‰
                return url
            
            elif strategy == "imgbed":
                # å›¾åºŠæ¨¡å¼ï¼šä¿å­˜åˆ°æœ¬åœ°å¹¶ç”ŸæˆURL
                filepath = self.save_to_local(compressed_data)
                return self.generate_url(filepath)
            
            else:  # smart
                # æ™ºèƒ½æ¨¡å¼ï¼šå…ˆå°è¯•è¿”å›åŸURLï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨å›¾åºŠ
                # ä¿å­˜åˆ°æœ¬åœ°ä½œä¸ºå¤‡ä»½
                filepath = self.save_to_local(compressed_data)
                
                # è¿”å›åŸURLå’Œæœ¬åœ°URLï¼ˆè®©è½¬å‘å™¨å†³å®šï¼‰
                return {
                    'original': url,
                    'local': self.generate_url(filepath),
                    'filepath': filepath
                }
            
        except Exception as e:
            logger.error(f"å›¾ç‰‡å¤„ç†å¤±è´¥: {str(e)}")
            return None
    
    def get_processing_stats(self) -> Dict[str, Any]:
        """
        è·å–å›¾ç‰‡å¤„ç†ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        avg_compressed_mb = (
            self.stats['total_compressed_mb'] / self.stats['total_processed']
            if self.stats['total_processed'] > 0 else 0
        )
        avg_saved_mb = (
            self.stats['total_saved_mb'] / self.stats['total_processed']
            if self.stats['total_processed'] > 0 else 0
        )
        
        return {
            'total_processed': self.stats['total_processed'],
            'total_compressed_mb': round(self.stats['total_compressed_mb'], 2),
            'total_saved_mb': round(self.stats['total_saved_mb'], 2),
            'parallel_batch_count': self.stats['parallel_count'],
            'avg_compressed_mb': round(avg_compressed_mb, 2),
            'avg_saved_mb': round(avg_saved_mb, 2),
            'process_pool_workers': self.process_pool._max_workers if hasattr(self.process_pool, '_max_workers') else 0
        }
    
    def shutdown(self):
        """å…³é—­è¿›ç¨‹æ± """
        try:
            self.process_pool.shutdown(wait=True)
            logger.info("å›¾ç‰‡å¤„ç†è¿›ç¨‹æ± å·²å…³é—­")
        except Exception as e:
            logger.error(f"å…³é—­è¿›ç¨‹æ± å¤±è´¥: {str(e)}")
    
    def __del__(self):
        """ææ„å‡½æ•°ï¼Œç¡®ä¿è¿›ç¨‹æ± è¢«å…³é—­"""
        self.shutdown()


class AttachmentProcessor:
    """é™„ä»¶æ–‡ä»¶å¤„ç†å™¨ï¼ˆæ”¯æŒæœ€å¤§50MBæ–‡ä»¶ï¼‰"""
    
    def __init__(self):
        # é™„ä»¶å­˜å‚¨ç›®å½•
        self.storage_path = Path(settings.data_dir) / "attachments"
        self.storage_path.mkdir(parents=True, exist_ok=True)
        
        # æœ€å¤§æ–‡ä»¶å¤§å°ï¼ˆ50MBï¼‰
        self.max_size_mb = 50
    
    async def download_attachment(self, url: str,
                                  filename: str,
                                  cookies: Optional[Dict] = None,
                                  referer: Optional[str] = None) -> Optional[str]:
        """
        ä¸‹è½½é™„ä»¶æ–‡ä»¶
        
        Args:
            url: é™„ä»¶URL
            filename: æ–‡ä»¶å
            cookies: Cookieå­—å…¸
            referer: Refererå¤´
            
        Returns:
            æœ¬åœ°æ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            logger.info(f"å¼€å§‹ä¸‹è½½é™„ä»¶: {filename}")
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            if referer:
                headers['Referer'] = referer
            
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    url,
                    headers=headers,
                    cookies=cookies,
                    timeout=aiohttp.ClientTimeout(total=60)  # 60ç§’è¶…æ—¶
                ) as response:
                    if response.status != 200:
                        logger.error(f"é™„ä»¶ä¸‹è½½å¤±è´¥: {url}, çŠ¶æ€ç : {response.status}")
                        return None
                    
                    # æ£€æŸ¥æ–‡ä»¶å¤§å°
                    content_length = response.headers.get('Content-Length')
                    if content_length:
                        size_mb = int(content_length) / (1024 * 1024)
                        if size_mb > self.max_size_mb:
                            logger.error(f"é™„ä»¶è¿‡å¤§: {size_mb:.2f}MB > {self.max_size_mb}MB")
                            return None
                        logger.info(f"é™„ä»¶å¤§å°: {size_mb:.2f}MB")
                    
                    # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å
                    safe_filename = self._sanitize_filename(filename)
                    
                    # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œæ·»åŠ æ—¶é—´æˆ³
                    filepath = self.storage_path / safe_filename
                    if filepath.exists():
                        name, ext = os.path.splitext(safe_filename)
                        timestamp = int(time.time())
                        safe_filename = f"{name}_{timestamp}{ext}"
                        filepath = self.storage_path / safe_filename
                    
                    # åˆ†å—ä¸‹è½½å¹¶ä¿å­˜
                    total_size = 0
                    with open(filepath, 'wb') as f:
                        async for chunk in response.content.iter_chunked(8192):
                            f.write(chunk)
                            total_size += len(chunk)
                            
                            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å¤§å¤§å°
                            if total_size > self.max_size_mb * 1024 * 1024:
                                f.close()
                                filepath.unlink()  # åˆ é™¤éƒ¨åˆ†ä¸‹è½½çš„æ–‡ä»¶
                                logger.error(f"é™„ä»¶ä¸‹è½½è¶…è¿‡æœ€å¤§é™åˆ¶: {self.max_size_mb}MB")
                                return None
                    
                    logger.info(f"âœ… é™„ä»¶ä¸‹è½½æˆåŠŸ: {filepath}, å¤§å°: {total_size / 1024:.2f}KB")
                    return str(filepath)
                    
        except asyncio.TimeoutError:
            logger.error(f"é™„ä»¶ä¸‹è½½è¶…æ—¶: {url}")
            return None
        except Exception as e:
            logger.error(f"é™„ä»¶ä¸‹è½½å¼‚å¸¸: {url}, é”™è¯¯: {str(e)}")
            return None
    
    def _sanitize_filename(self, filename: str) -> str:
        """
        æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤ä¸å®‰å…¨å­—ç¬¦
        
        Args:
            filename: åŸå§‹æ–‡ä»¶å
            
        Returns:
            å®‰å…¨çš„æ–‡ä»¶å
        """
        # ç§»é™¤è·¯å¾„åˆ†éš”ç¬¦å’Œä¸å®‰å…¨å­—ç¬¦
        unsafe_chars = ['/', '\\', ':', '*', '?', '"', '<', '>', '|']
        safe_name = filename
        for char in unsafe_chars:
            safe_name = safe_name.replace(char, '_')
        
        # é™åˆ¶æ–‡ä»¶åé•¿åº¦ï¼ˆä¿ç•™æ‰©å±•åï¼‰
        name, ext = os.path.splitext(safe_name)
        if len(name) > 200:
            name = name[:200]
        
        return name + ext
    
    async def cleanup_old_attachments(self, days: int = 7):
        """
        æ¸…ç†æ—§é™„ä»¶
        
        Args:
            days: ä¿ç•™å¤©æ•°
        """
        try:
            current_time = time.time()
            deleted_count = 0
            freed_space = 0
            
            for filepath in self.storage_path.glob("*"):
                if filepath.is_file():
                    # æ£€æŸ¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´
                    mtime = filepath.stat().st_mtime
                    age_days = (current_time - mtime) / (24 * 3600)
                    
                    if age_days > days:
                        file_size = filepath.stat().st_size
                        filepath.unlink()
                        deleted_count += 1
                        freed_space += file_size
            
            freed_mb = freed_space / (1024 * 1024)
            logger.info(f"æ¸…ç†æ—§é™„ä»¶å®Œæˆ: åˆ é™¤ {deleted_count} ä¸ªæ–‡ä»¶, é‡Šæ”¾ {freed_mb:.2f}MB ç©ºé—´")
            
        except Exception as e:
            logger.error(f"æ¸…ç†æ—§é™„ä»¶å¤±è´¥: {str(e)}")
    
    def get_storage_size(self) -> float:
        """
        è·å–é™„ä»¶å­˜å‚¨ç©ºé—´å ç”¨ï¼ˆGBï¼‰
        
        Returns:
            å ç”¨å¤§å°ï¼ˆGBï¼‰
        """
        total_size = 0
        for filepath in self.storage_path.glob("*"):
            if filepath.is_file():
                total_size += filepath.stat().st_size
        
        return total_size / (1024 ** 3)
    
    def get_file_info(self, filepath: str) -> Optional[Dict]:
        """
        è·å–æ–‡ä»¶ä¿¡æ¯
        
        Args:
            filepath: æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ–‡ä»¶ä¿¡æ¯å­—å…¸
        """
        try:
            path = Path(filepath)
            if not path.exists():
                return None
            
            stat = path.stat()
            return {
                'filename': path.name,
                'size': stat.st_size,
                'size_mb': stat.st_size / (1024 * 1024),
                'created_at': stat.st_ctime,
                'modified_at': stat.st_mtime,
            }
        except Exception as e:
            logger.error(f"è·å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥: {str(e)}")
            return None


# å…¨å±€å•ä¾‹
image_processor = ImageProcessor()
attachment_processor = AttachmentProcessor()
