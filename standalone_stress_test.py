#!/usr/bin/env python3
"""
KOOKæ¶ˆæ¯è½¬å‘ç³»ç»Ÿ - ç‹¬ç«‹å‹åŠ›æµ‹è¯•
å®Œå…¨ç‹¬ç«‹ï¼Œæ— éœ€ä»»ä½•ä¾èµ–
"""

import time
import json
import re
import asyncio
import statistics
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any
from collections import deque


# ============ æ¨¡æ‹Ÿæ ¸å¿ƒåŠŸèƒ½æ¨¡å— ============

class MessageFormatter:
    """æ¶ˆæ¯æ ¼å¼è½¬æ¢å™¨ï¼ˆç‹¬ç«‹ç‰ˆæœ¬ï¼‰"""
    
    EMOJI_MAP = {
        "å¼€å¿ƒ": "ğŸ˜Š", "ç¬‘": "ğŸ˜„", "å¤§ç¬‘": "ğŸ˜†", "å“ˆå“ˆ": "ğŸ˜‚",
        "å“­": "ğŸ˜­", "ä¼¤å¿ƒ": "ğŸ˜", "ç”Ÿæ°”": "ğŸ˜ ", "çˆ±å¿ƒ": "â¤ï¸",
        "èµ": "ğŸ‘", "ç«": "ğŸ”¥", "æ˜Ÿæ˜Ÿ": "â­", "é’±": "ğŸ’°"
    }
    
    @staticmethod
    def kmarkdown_to_discord(text: str) -> str:
        """KMarkdownè½¬Discord Markdown"""
        if not text:
            return ""
        
        # è½¬æ¢è¡¨æƒ…
        text = re.sub(
            r'\(emj\)(\w+)\(emj\)',
            lambda m: MessageFormatter.EMOJI_MAP.get(m.group(1), f":{m.group(1)}:"),
            text
        )
        
        return text
    
    @staticmethod
    def kmarkdown_to_telegram_html(text: str) -> str:
        """KMarkdownè½¬Telegram HTML"""
        if not text:
            return ""
        
        # è½¬æ¢è¡¨æƒ…
        text = re.sub(
            r'\(emj\)(\w+)\(emj\)',
            lambda m: MessageFormatter.EMOJI_MAP.get(m.group(1), f":{m.group(1)}:"),
            text
        )
        
        # è½¬æ¢æ ¼å¼
        text = re.sub(r'\*\*(.+?)\*\*', r'<b>\1</b>', text)
        text = re.sub(r'\*(.+?)\*', r'<i>\1</i>', text)
        text = re.sub(r'`(.+?)`', r'<code>\1</code>', text)
        
        return text
    
    @staticmethod
    def split_long_message(text: str, max_length: int) -> list:
        """æ™ºèƒ½åˆ†å‰²è¶…é•¿æ¶ˆæ¯"""
        if len(text) <= max_length:
            return [text]
        
        messages = []
        current = ""
        
        for char in text:
            if len(current) + 1 <= max_length:
                current += char
            else:
                messages.append(current)
                current = char
        
        if current:
            messages.append(current)
        
        return messages


class RateLimiter:
    """é™æµå™¨ï¼ˆç‹¬ç«‹ç‰ˆæœ¬ï¼‰"""
    
    def __init__(self, calls: int, period: int):
        self.calls = calls
        self.period = period
        self.timestamps = deque()
    
    async def acquire(self):
        """è·å–è®¸å¯"""
        now = time.time()
        
        # æ¸…ç†è¿‡æœŸæ—¶é—´æˆ³
        while self.timestamps and self.timestamps[0] < now - self.period:
            self.timestamps.popleft()
        
        if len(self.timestamps) >= self.calls:
            # éœ€è¦ç­‰å¾…
            sleep_time = self.period - (now - self.timestamps[0])
            if sleep_time > 0:
                await asyncio.sleep(sleep_time)
            return await self.acquire()
        
        self.timestamps.append(now)


# ============ å‹åŠ›æµ‹è¯•ç±» ============

class StandaloneStressTest:
    """ç‹¬ç«‹å‹åŠ›æµ‹è¯•"""
    
    def __init__(self):
        self.formatter = MessageFormatter()
        self.results = {
            "test_time": datetime.now().isoformat(),
            "test_type": "ç‹¬ç«‹å‹åŠ›æµ‹è¯•",
            "total_tests": 0,
            "passed": 0,
            "failed": 0,
            "performance_metrics": {}
        }
    
    def print_header(self, title: str):
        """æ‰“å°æ ‡é¢˜"""
        print("\n" + "=" * 80)
        print(f"  {title}")
        print("=" * 80 + "\n")
    
    def print_result(self, name: str, value: float, unit: str = ""):
        """æ‰“å°ç»“æœ"""
        print(f"  âœ“ {name}: {value:,.2f} {unit}")
    
    # ========== æµ‹è¯•1: æ¶ˆæ¯æ ¼å¼è½¬æ¢ ==========
    
    def test_message_formatting(self):
        """æµ‹è¯•æ¶ˆæ¯æ ¼å¼è½¬æ¢"""
        self.print_header("æµ‹è¯•1: æ¶ˆæ¯æ ¼å¼è½¬æ¢æ€§èƒ½")
        
        test_texts = [
            "**ç²—ä½“** *æ–œä½“* `ä»£ç `",
            "(emj)å¼€å¿ƒ(emj) (emj)ç¬‘(emj) @ç”¨æˆ·å",
            "è¿™æ˜¯ä¸€æ®µå¾ˆé•¿çš„æ–‡æœ¬ï¼Œç”¨äºæµ‹è¯•æ€§èƒ½ã€‚" * 10,
            "~~åˆ é™¤çº¿~~ [é“¾æ¥](https://example.com)",
        ]
        
        metrics = []
        
        # Discordè½¬æ¢
        for iterations in [1000, 5000, 10000, 50000, 100000]:
            start = time.time()
            for _ in range(iterations):
                for text in test_texts:
                    self.formatter.kmarkdown_to_discord(text)
            elapsed = time.time() - start
            ops_per_sec = (iterations * len(test_texts)) / elapsed
            
            metrics.append({
                "test": f"Discordæ ¼å¼è½¬æ¢({iterations}æ¬¡)",
                "ops_per_second": ops_per_sec,
                "elapsed": elapsed
            })
            
            print(f"â„¹ï¸  Discordæ ¼å¼è½¬æ¢ ({iterations:,}æ¬¡)")
            self.print_result("æ€§èƒ½", ops_per_sec, "ops/s")
            self.print_result("è€—æ—¶", elapsed, "ç§’")
            
            self.results["total_tests"] += 1
            if ops_per_sec > 10000:
                self.results["passed"] += 1
            else:
                self.results["failed"] += 1
        
        # Telegramè½¬æ¢
        for iterations in [10000, 50000]:
            start = time.time()
            for _ in range(iterations):
                for text in test_texts:
                    self.formatter.kmarkdown_to_telegram_html(text)
            elapsed = time.time() - start
            ops_per_sec = (iterations * len(test_texts)) / elapsed
            
            metrics.append({
                "test": f"Telegramæ ¼å¼è½¬æ¢({iterations}æ¬¡)",
                "ops_per_second": ops_per_sec,
                "elapsed": elapsed
            })
            
            print(f"â„¹ï¸  Telegramæ ¼å¼è½¬æ¢ ({iterations:,}æ¬¡)")
            self.print_result("æ€§èƒ½", ops_per_sec, "ops/s")
            
            self.results["total_tests"] += 1
            if ops_per_sec > 8000:
                self.results["passed"] += 1
            else:
                self.results["failed"] += 1
        
        # æ¶ˆæ¯åˆ†æ®µ
        long_text = "è¿™æ˜¯ä¸€æ®µå¾ˆé•¿çš„æ–‡æœ¬ã€‚" * 200
        iterations = 10000
        start = time.time()
        for _ in range(iterations):
            self.formatter.split_long_message(long_text, 2000)
        elapsed = time.time() - start
        ops_per_sec = iterations / elapsed
        
        metrics.append({
            "test": f"æ¶ˆæ¯æ™ºèƒ½åˆ†æ®µ({iterations}æ¬¡)",
            "ops_per_second": ops_per_sec,
            "elapsed": elapsed
        })
        
        print(f"â„¹ï¸  æ¶ˆæ¯æ™ºèƒ½åˆ†æ®µ ({iterations:,}æ¬¡)")
        self.print_result("æ€§èƒ½", ops_per_sec, "ops/s")
        
        self.results["total_tests"] += 1
        if ops_per_sec > 1000:
            self.results["passed"] += 1
        else:
            self.results["failed"] += 1
        
        self.results["performance_metrics"]["message_formatting"] = metrics
        print("âœ… æ¶ˆæ¯æ ¼å¼è½¬æ¢æµ‹è¯•å®Œæˆ\n")
    
    # ========== æµ‹è¯•2: é™æµå™¨ ==========
    
    async def test_rate_limiter(self):
        """æµ‹è¯•é™æµå™¨"""
        self.print_header("æµ‹è¯•2: é™æµå™¨å‡†ç¡®æ€§")
        
        test_configs = [
            {"calls": 5, "period": 1, "name": "Discordé™æµ(5/1s)"},
            {"calls": 30, "period": 1, "name": "Telegramé™æµ(30/1s)"},
            {"calls": 20, "period": 1, "name": "é£ä¹¦é™æµ(20/1s)"},
            {"calls": 100, "period": 5, "name": "è‡ªå®šä¹‰é™æµ(100/5s)"},
        ]
        
        metrics = []
        
        for config in test_configs:
            limiter = RateLimiter(config["calls"], config["period"])
            
            start = time.time()
            for _ in range(config["calls"]):
                await limiter.acquire()
            elapsed = time.time() - start
            
            accuracy = min(100, (config["period"] / elapsed) * 100)
            
            metrics.append({
                "test": config["name"],
                "expected": config["period"],
                "actual": elapsed,
                "accuracy": accuracy
            })
            
            print(f"â„¹ï¸  {config['name']}")
            print(f"  âœ“ é¢„æœŸ: {config['period']:.2f}s, å®é™…: {elapsed:.2f}s")
            self.print_result("å‡†ç¡®åº¦", accuracy, "%")
            
            self.results["total_tests"] += 1
            if accuracy > 95:
                self.results["passed"] += 1
            else:
                self.results["failed"] += 1
        
        self.results["performance_metrics"]["rate_limiter"] = metrics
        print("âœ… é™æµå™¨æµ‹è¯•å®Œæˆ\n")
    
    # ========== æµ‹è¯•3: å¹¶å‘å¤„ç† ==========
    
    async def test_concurrent_processing(self):
        """æµ‹è¯•å¹¶å‘å¤„ç†"""
        self.print_header("æµ‹è¯•3: å¹¶å‘å¤„ç†èƒ½åŠ›")
        
        async def process_message(msg_id: int) -> bool:
            """æ¨¡æ‹Ÿæ¶ˆæ¯å¤„ç†"""
            text = f"**æ¶ˆæ¯{msg_id}** å†…å®¹æµ‹è¯• (emj)å¼€å¿ƒ(emj)"
            self.formatter.kmarkdown_to_discord(text)
            await asyncio.sleep(0.001)  # æ¨¡æ‹ŸI/O
            return True
        
        metrics = []
        
        for level in [10, 50, 100, 200, 500, 1000]:
            start = time.time()
            tasks = [process_message(i) for i in range(level)]
            results = await asyncio.gather(*tasks)
            elapsed = time.time() - start
            
            throughput = level / elapsed
            success_rate = (sum(results) / len(results)) * 100
            
            metrics.append({
                "concurrent_level": level,
                "throughput": throughput,
                "elapsed": elapsed,
                "success_rate": success_rate
            })
            
            print(f"â„¹ï¸  å¹¶å‘çº§åˆ«: {level}")
            self.print_result("ååé‡", throughput, "msg/s")
            self.print_result("æˆåŠŸç‡", success_rate, "%")
            
            self.results["total_tests"] += 1
            if throughput > 50:
                self.results["passed"] += 1
            else:
                self.results["failed"] += 1
        
        self.results["performance_metrics"]["concurrent_processing"] = metrics
        print("âœ… å¹¶å‘å¤„ç†æµ‹è¯•å®Œæˆ\n")
    
    # ========== æµ‹è¯•4: é˜Ÿåˆ—æ€§èƒ½ ==========
    
    def test_queue_performance(self):
        """æµ‹è¯•é˜Ÿåˆ—æ€§èƒ½"""
        self.print_header("æµ‹è¯•4: é˜Ÿåˆ—æ€§èƒ½")
        
        metrics = []
        
        for batch_size in [100, 500, 1000, 5000, 10000, 50000]:
            # å…¥é˜Ÿ
            queue = deque()
            messages = [{"id": i, "content": f"æ¶ˆæ¯{i}"} for i in range(batch_size)]
            
            start = time.time()
            for msg in messages:
                queue.append(json.dumps(msg))
            elapsed = time.time() - start
            enqueue_qps = batch_size / elapsed if elapsed > 0 else 0
            
            # å‡ºé˜Ÿ
            start = time.time()
            while queue:
                msg = queue.popleft()
                json.loads(msg)
            elapsed = time.time() - start
            dequeue_qps = batch_size / elapsed if elapsed > 0 else 0
            
            metrics.append({
                "batch_size": batch_size,
                "enqueue_qps": enqueue_qps,
                "dequeue_qps": dequeue_qps
            })
            
            print(f"â„¹ï¸  æ‰¹é‡å¤§å°: {batch_size:,}")
            self.print_result("å…¥é˜ŸQPS", enqueue_qps, "msg/s")
            self.print_result("å‡ºé˜ŸQPS", dequeue_qps, "msg/s")
            
            self.results["total_tests"] += 1
            if enqueue_qps > 10000 and dequeue_qps > 10000:
                self.results["passed"] += 1
            else:
                self.results["failed"] += 1
        
        self.results["performance_metrics"]["queue_performance"] = metrics
        print("âœ… é˜Ÿåˆ—æ€§èƒ½æµ‹è¯•å®Œæˆ\n")
    
    # ========== æµ‹è¯•5: JSONåºåˆ—åŒ– ==========
    
    def test_json_serialization(self):
        """æµ‹è¯•JSONåºåˆ—åŒ–"""
        self.print_header("æµ‹è¯•5: JSONåºåˆ—åŒ–æ€§èƒ½")
        
        test_data = {
            "message_id": "msg_12345",
            "channel_id": "channel_67890",
            "content": "è¿™æ˜¯ä¸€æ¡æµ‹è¯•æ¶ˆæ¯ (emj)å¼€å¿ƒ(emj)",
            "sender": "ç”¨æˆ·A",
            "timestamp": 1698765432,
            "attachments": [
                {"type": "image", "url": "https://example.com/image1.jpg"},
                {"type": "file", "url": "https://example.com/file1.pdf"}
            ]
        }
        
        metrics = []
        
        # åºåˆ—åŒ–æµ‹è¯•
        iterations = 100000
        start = time.time()
        for _ in range(iterations):
            json.dumps(test_data, ensure_ascii=False)
        elapsed = time.time() - start
        serialize_qps = iterations / elapsed
        
        # ååºåˆ—åŒ–æµ‹è¯•
        json_str = json.dumps(test_data, ensure_ascii=False)
        start = time.time()
        for _ in range(iterations):
            json.loads(json_str)
        elapsed = time.time() - start
        deserialize_qps = iterations / elapsed
        
        metrics.append({
            "iterations": iterations,
            "serialize_qps": serialize_qps,
            "deserialize_qps": deserialize_qps
        })
        
        print(f"â„¹ï¸  æµ‹è¯•æ¬¡æ•°: {iterations:,}")
        self.print_result("åºåˆ—åŒ–QPS", serialize_qps, "ops/s")
        self.print_result("ååºåˆ—åŒ–QPS", deserialize_qps, "ops/s")
        
        self.results["total_tests"] += 1
        if serialize_qps > 10000 and deserialize_qps > 10000:
            self.results["passed"] += 1
        else:
            self.results["failed"] += 1
        
        self.results["performance_metrics"]["json_serialization"] = metrics
        print("âœ… JSONåºåˆ—åŒ–æµ‹è¯•å®Œæˆ\n")
    
    # ========== ä¸»æµ‹è¯•å‡½æ•° ==========
    
    async def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("=" * 80)
        print("  KOOKæ¶ˆæ¯è½¬å‘ç³»ç»Ÿ - ç‹¬ç«‹å‹åŠ›æµ‹è¯•")
        print("=" * 80)
        print(f"\næµ‹è¯•æ—¶é—´: {self.results['test_time']}")
        print("è¯´æ˜: å®Œå…¨ç‹¬ç«‹è¿è¡Œï¼Œæ— éœ€ä»»ä½•å¤–éƒ¨ä¾èµ–\n")
        
        # æ‰§è¡Œæ‰€æœ‰æµ‹è¯•
        self.test_message_formatting()
        await self.test_rate_limiter()
        await self.test_concurrent_processing()
        self.test_queue_performance()
        self.test_json_serialization()
        
        # æ‰“å°æ€»ç»“
        self.print_summary()
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_report()
    
    def print_summary(self):
        """æ‰“å°æ€»ç»“"""
        self.print_header("æµ‹è¯•æ€»ç»“")
        
        print(f"æµ‹è¯•æ—¶é—´: {self.results['test_time']}")
        print(f"æ€»æµ‹è¯•æ•°: {self.results['total_tests']}")
        print(f"é€šè¿‡æ•°é‡: {self.results['passed']}")
        print(f"å¤±è´¥æ•°é‡: {self.results['failed']}")
        
        if self.results['total_tests'] > 0:
            success_rate = (self.results['passed'] / self.results['total_tests']) * 100
            print(f"æˆåŠŸç‡: {success_rate:.2f}%")
        
        # æ€§èƒ½äº®ç‚¹
        print("\n" + "=" * 80)
        print("  æ€§èƒ½äº®ç‚¹")
        print("=" * 80 + "\n")
        
        # æ ¼å¼è½¬æ¢æœ€ä½³æ€§èƒ½
        if "message_formatting" in self.results["performance_metrics"]:
            metrics = self.results["performance_metrics"]["message_formatting"]
            max_ops = max(m["ops_per_second"] for m in metrics)
            print(f"âœ¨ æ¶ˆæ¯æ ¼å¼è½¬æ¢å³°å€¼: {max_ops:,.0f} ops/s")
        
        # é˜Ÿåˆ—æœ€ä½³æ€§èƒ½
        if "queue_performance" in self.results["performance_metrics"]:
            metrics = self.results["performance_metrics"]["queue_performance"]
            max_enqueue = max(m["enqueue_qps"] for m in metrics)
            max_dequeue = max(m["dequeue_qps"] for m in metrics)
            print(f"âœ¨ é˜Ÿåˆ—å…¥é˜Ÿå³°å€¼: {max_enqueue:,.0f} msg/s")
            print(f"âœ¨ é˜Ÿåˆ—å‡ºé˜Ÿå³°å€¼: {max_dequeue:,.0f} msg/s")
        
        # å¹¶å‘æœ€ä½³æ€§èƒ½
        if "concurrent_processing" in self.results["performance_metrics"]:
            metrics = self.results["performance_metrics"]["concurrent_processing"]
            max_throughput = max(m["throughput"] for m in metrics)
            print(f"âœ¨ å¹¶å‘å¤„ç†å³°å€¼: {max_throughput:,.0f} msg/s")
        
        print()
    
    def generate_report(self):
        """ç”ŸæˆæŠ¥å‘Š"""
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        report_dir = Path("test_results")
        report_dir.mkdir(parents=True, exist_ok=True)
        
        # JSONæŠ¥å‘Š
        json_path = report_dir / "standalone_stress_test_report.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… JSONæŠ¥å‘Šå·²ä¿å­˜: {json_path}")
        
        # MarkdownæŠ¥å‘Š
        md_path = report_dir / "ç‹¬ç«‹å‹åŠ›æµ‹è¯•æŠ¥å‘Š.md"
        self.generate_markdown_report(md_path)
        print(f"âœ… MarkdownæŠ¥å‘Šå·²ä¿å­˜: {md_path}")
    
    def generate_markdown_report(self, output_path: Path):
        """ç”ŸæˆMarkdownæŠ¥å‘Š"""
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write("# KOOKæ¶ˆæ¯è½¬å‘ç³»ç»Ÿ - ç‹¬ç«‹å‹åŠ›æµ‹è¯•æŠ¥å‘Š\n\n")
            f.write(f"**æµ‹è¯•æ—¶é—´**: {self.results['test_time']}  \n")
            f.write(f"**æµ‹è¯•ç±»å‹**: {self.results['test_type']}  \n\n")
            
            f.write("## ğŸ“Š æµ‹è¯•æ€»ç»“\n\n")
            f.write("| æŒ‡æ ‡ | æ•°å€¼ |\n")
            f.write("|------|------|\n")
            f.write(f"| æ€»æµ‹è¯•æ•° | {self.results['total_tests']} |\n")
            f.write(f"| é€šè¿‡æ•°é‡ | {self.results['passed']} |\n")
            f.write(f"| å¤±è´¥æ•°é‡ | {self.results['failed']} |\n")
            
            if self.results['total_tests'] > 0:
                success_rate = (self.results['passed'] / self.results['total_tests']) * 100
                f.write(f"| æˆåŠŸç‡ | {success_rate:.2f}% |\n")
            
            f.write("\n## ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡\n\n")
            
            for category, metrics in self.results["performance_metrics"].items():
                f.write(f"### {category}\n\n")
                
                if metrics:
                    f.write("| æµ‹è¯•é¡¹ | æ€§èƒ½æŒ‡æ ‡ |\n")
                    f.write("|--------|----------|\n")
                    
                    for metric in metrics[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                        metric_str = json.dumps(metric, ensure_ascii=False)
                        f.write(f"| {metric.get('test', category)} | {metric_str} |\n")
                
                f.write("\n")


async def main():
    """ä¸»å‡½æ•°"""
    tester = StandaloneStressTest()
    await tester.run_all_tests()


if __name__ == "__main__":
    asyncio.run(main())
